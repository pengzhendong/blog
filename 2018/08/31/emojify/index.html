<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!0,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"0C48U14D5U",apiKey:"796da148bd60fa95ebae037cae6d5c16",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 写论文做实验的时候曾经想过用文本分类的模型，无奈样本太不均衡，所以最后用了自编码器提取特征。在 Coursera 的作业中，该实验分为两个小实验，一个是普通的文本分类，一个是使用 LSTM RNN 进行文本分类。"><meta property="og:type" content="article"><meta property="og:title" content="Emojify 文本情感分析"><meta property="og:url" content="https://pengzhendong.github.io/2018/08/31/emojify/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 写论文做实验的时候曾经想过用文本分类的模型，无奈样本太不均衡，所以最后用了自编码器提取特征。在 Coursera 的作业中，该实验分为两个小实验，一个是普通的文本分类，一个是使用 LSTM RNN 进行文本分类。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pengzhendong.github.io/2018/08/31/emojify/data_set.png"><meta property="og:image" content="https://pengzhendong.github.io/2018/08/31/emojify/emo_model.png"><meta property="og:image" content="https://pengzhendong.github.io/2018/08/31/emojify/output.png"><meta property="og:image" content="https://pengzhendong.github.io/2018/08/31/emojify/emojifier-v2.png"><meta property="article:published_time" content="2018-08-31T10:18:33.000Z"><meta property="article:modified_time" content="2018-08-31T12:18:57.000Z"><meta property="article:author" content="Randy Peng"><meta property="article:tag" content="Deep Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pengzhendong.github.io/2018/08/31/emojify/data_set.png"><link rel="canonical" href="https://pengzhendong.github.io/2018/08/31/emojify/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>Emojify 文本情感分析 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.github.io/2018/08/31/emojify/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Emojify 文本情感分析</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-08-31 10:18:33 / 修改时间：12:18:57" itemprop="dateCreated datePublished" datetime="2018-08-31T10:18:33+00:00">2018-08-31</time></span><span id="/2018/08/31/emojify/" class="post-meta-item leancloud_visitors" data-flag-title="Emojify 文本情感分析" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>7.5k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>写论文做实验的时候曾经想过用文本分类的模型，无奈样本太不均衡，所以最后用了自编码器提取特征。在 Coursera 的作业中，该实验分为两个小实验，一个是普通的文本分类，一个是使用 LSTM RNN 进行文本分类。</p><span id="more"></span><h2 id="baseline-模型-emojifier-v1">Baseline 模型: Emojifier-V1</h2><p>训练集 X 中包含 127 个句子，其标签为 0 到 4 分别对应一个 emoji 表情，如下图所示：</p><p><img src="data_set.png"></p><p>现在载入数据集，并且测试一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_train, Y_train = read_csv(<span class="string">'data/train_emoji.csv'</span>)</span><br><span class="line">X_test, Y_test = read_csv(<span class="string">'data/tesss.csv'</span>)</span><br><span class="line">maxLen = <span class="built_in">len</span>(<span class="built_in">max</span>(X_train, key=<span class="built_in">len</span>).split())</span><br><span class="line"></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(X_train[index], label_to_emoji(Y_train[index]))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">I am proud of your achievements 😄</span><br></pre></td></tr></table></figure><h3 id="emojifier-v1-概况">Emojifier-V1 概况</h3><p>Emojifier-V1 的概况如下图所示：</p><p><img src="emo_model.png"></p><p>该模型比较简单，首先去训练好的 Embedding 中找到每个单词的嵌入，然后对句子中所有单词的嵌入求平均，将其作为输入，输入到一个多分类的全连接网络中，最后预测句子的情感。</p><h3 id="实现-emojifier-v1">实现 Emojifier-V1</h3><p>此处不再细述多分类的过程，模型的主要内容如下所示：<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align:-.464ex" xmlns="http://www.w3.org/2000/svg" width="20.284ex" height="2.598ex" role="img" focusable="false" viewBox="0 -943.3 8965.6 1148.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346 162 335 155 324L153 320Q150 317 138 317 117 317 117 325 117 330 120 339 133 378 163 406T229 440Q241 442 246 442 271 442 291 425T329 392 367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434 468 430 463 420T449 399 432 377 418 358L411 349Q368 298 275 214T160 106L148 94 163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143 443 138 442 134 425 72 376 31T278-11Q252-11 232 6T193 40 155 57Q111 57 76-3 70-11 59-11H54 41Q35-5 35-2 35 13 93 84 132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="TeXAtom" transform="translate(498,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1619.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347 722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153 722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2675.6,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674 695 670 692 659 687 641 683 639T661 637Q636 636 621 632T600 624 597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170 666 200 690 241 720 295 759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647 817 650 819 660 823 676 825 679T839 682Q842 682 856 682T895 682 949 681Q1015 681 1034 683 1048 683 1048 672 1048 666 1045 655T1038 640 1028 637Q1006 637 988 631T958 617 939 600 927 584L923 578 754 282Q586-14 585-15 579-22 561-22 546-22 542-17 539-14 523 229T506 480L494 462Q472 425 366 239 222-13 220-15T215-19Q210-22 197-22 178-22 176-15 176-12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648 52 671 64 683H76Q118 680 176 680 301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620 262 160 266 136L501 550 499 587Q496 629 489 632 483 636 447 637 428 637 422 639T416 648Q416 650 418 660 419 664 420 669T421 676 424 680 428 682 436 683Z"></path></g><g data-mml-node="mo" transform="translate(3945.9,0)"><path data-c="D7" d="M630 29Q630 9 609 9 604 9 587 25T493 118L389 222 284 117Q178 13 175 11 171 9 168 9 160 9 154 15T147 29Q147 36 161 51T255 146L359 250 255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489 178 487 284 383L389 278 493 382Q570 459 587 475T609 491Q630 491 630 471 630 464 620 453T522 355L418 250 522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(4946.1,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392 386 422 416 422 429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487Q506 153 506 144 506 138 501 117T481 63 449 13Q436 0 417-8 409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157ZM351 328Q351 334 346 350T323 385 277 405Q242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(5475.1,0)"><path data-c="1D463" d="M173 380Q173 405 154 405 130 405 104 376T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287 21 294 29 316T53 368 97 419 160 441Q202 441 225 417T249 361Q249 344 246 335 246 329 231 291T200 202 182 113Q182 86 187 69 200 26 250 26 287 26 319 60T369 139 398 222 409 277Q409 300 401 317T383 343 365 361 357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159 347 40 241-11Q177-11 139 22 102 54 102 117 102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="msup" transform="translate(5960.1,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412 431 419 447 422Q461 422 470 413T480 394Q480 379 423 152T363-80Q345-134 286-169T151-205Q10-205 10-137 10-111 28-91T74-71Q89-71 102-80T116-111Q116-121 114-130T107-144 99-154 92-162L90-164H91Q101-167 151-167 189-167 211-155 234-144 254-122T282-75Q288-56 298-13 311 35 311 43ZM384 328 380 339Q377 350 375 354T369 368 359 382 346 393 328 402 306 405Q262 405 221 352 191 313 171 233T151 117Q151 38 213 38 269 38 323 108L331 118 384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(510,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7536.4,0)"><path data-c="2B" d="M56 237T56 250 70 270H369V420L370 570Q380 583 389 583 402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401-82 391-82H389 387Q375-82 369-68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(8536.6,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402 231 442 283 442 345 442 383 396T422 280Q422 169 343 79T173-11Q123-11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640 73 647ZM336 325V331Q336 405 275 405 258 405 240 397T207 376 181 352 163 330L157 322 136 236Q114 150 114 114 114 66 138 42 154 26 178 26 211 26 245 58 270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container></span></p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align:-.566ex" xmlns="http://www.w3.org/2000/svg" width="19.692ex" height="2.7ex" role="img" focusable="false" viewBox="0 -943.3 8703.7 1193.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392 386 422 416 422 429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487Q506 153 506 144 506 138 501 117T481 63 449 13Q436 0 417-8 409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157ZM351 328Q351 334 346 350T323 385 277 405Q242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1683.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347 722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153 722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2739.6,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415 300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372 367 378Q368 378 368 379 368 382 361 388T336 399 297 405Q249 405 227 379T204 326Q204 301 223 291T278 274 330 259Q396 230 396 163 396 135 385 107T352 51 289 7 195-10Q118-10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34 201 27Q237 27 263 38T301 66 318 97 323 122Q323 150 302 164T254 181 195 196 148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(3208.6,0)"><path data-c="1D45C" d="M201-11Q126-11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441 333 441 341 440 354 437 367 433T402 417 438 387 464 338 476 268Q476 161 390 75T201-11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375 346 405 306 405 243 405 195 347 158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(3693.6,0)"><path data-c="1D453" d="M118-162Q120-162 124-164T135-167 147-168Q160-168 171-155T187-126Q197-99 221 27T267 267 289 382V385H242Q195 385 192 387 188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432 298 434 307 482T319 540Q356 705 465 705 502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603 443 622 454 636T478 657L487 662Q471 668 457 668 445 668 434 658T419 630Q412 601 403 552T387 469 380 433Q380 431 435 431 480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282-47 255-132 212-173 175-205 139-205 107-205 81-186T55-132Q55-95 76-78T118-61Q162-61 162-103 162-122 151-136T127-157L118-162Z"></path></g><g data-mml-node="mi" transform="translate(4243.6,0)"><path data-c="1D461" d="M26 385Q19 392 19 395 19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566 179 586 187 603 197 615 211 624 229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420 330 398 317 385H210L174 240Q135 80 135 68 135 26 162 26 197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145 322 142 319 133 314 117 301 95T267 48 216 6 155-11Q125-11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383 128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(4604.6,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341 56 388 88 425 132 442 175 435 205 417 221 395 229 376L231 369Q231 367 232 367L243 378Q303 442 384 442 401 442 415 440T441 433 460 423 475 411 485 398 493 385 497 373 500 364 502 357L510 367Q573 442 659 442 713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145 857 144 853 130 845 101 831 73T785 17 716-10Q669-10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291 466 157Q433 26 428 16 415-11 385-11 372-11 364-4T353 8 350 18Q350 29 384 161L420 307Q423 322 423 345 423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 181 151 335 151 342 154 357 154 369 154 405 129 405 107 405 92 377T69 316 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5482.6,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392 386 422 416 422 429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487Q506 153 506 144 506 138 501 117T481 63 449 13Q436 0 417-8 409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157ZM351 328Q351 334 346 350T323 385 277 405Q242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(6011.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442 467 442 494 420T522 361Q522 332 508 314T481 292 458 288Q439 288 427 299T415 328Q415 374 465 391 454 404 425 404 412 404 406 402 368 386 350 336 290 115 290 78 290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145 504 144 502 134 486 77 440 33T333-11Q263-11 227 52 186-10 133-10H127Q78-10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101 142 81 130 66T107 46 94 41L91 40Q91 39 97 36T113 29 132 26Q168 26 194 71 203 87 217 139T245 247 261 313Q266 340 266 352 266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(6583.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(6972.6,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346 162 335 155 324L153 320Q150 317 138 317 117 317 117 325 117 330 120 339 133 378 163 406T229 440Q241 442 246 442 271 442 291 425T329 392 367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434 468 430 463 420T449 399 432 377 418 358L411 349Q368 298 275 214T160 106L148 94 163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143 443 138 442 134 425 72 376 31T278-11Q252-11 232 6T193 40 155 57Q111 57 76-3 70-11 59-11H54 41Q35-5 35-2 35 13 93 84 132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="TeXAtom" transform="translate(498,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(8314.7,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align:-2.864ex" xmlns="http://www.w3.org/2000/svg" width="28.808ex" height="7.028ex" role="img" focusable="false" viewBox="0 -1840.5 12733.2 3106.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62-22T47-22 32-11Q32-1 56 24T83 55Q113 96 138 172T180 320 234 473 323 609Q364 649 419 677T531 705Q559 705 578 696T604 671 615 645 618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520 503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617 367 578 333 492T271 301 233 170Q211 123 204 112L198 103 224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152 656 152 656 142 656 101 588 40T433-22Q381-22 289 1T156 28L141 29 131 20Q111 0 87-11Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(723,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1844.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347 722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153 722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2900.6,0)"><path data-c="2212" d="M84 237T84 250 98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="munderover" transform="translate(3845.3,0)"><g data-mml-node="mo" transform="translate(93.8,0)"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761 1235 809 1174 838T1033 881 882 898 699 902H574 543 251L259 891Q722 258 724 252 725 250 724 246 721 243 460-56L196-356Q196-357 407-357 459-357 548-357T676-358Q812-358 896-353T1063-332 1204-283 1307-196Q1328-170 1348-124H1388Q1388-125 1381-145T1356-210 1325-294L1267-449 666-450Q64-450 61-448 55-446 55-439 55-437 57-433L590 177Q590 178 557 222T452 366 322 544L56 909 55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(179.8,-1107.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686 294 679 244 477 194 279 194 272 213 282 223 291 247 309 292 354T362 415Q402 442 438 442 468 442 485 423T503 369Q503 344 496 327T477 302 456 291 438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316 228 255Q230 254 243 252T267 246 293 238 320 224 342 206 359 180 365 147Q365 130 360 106T354 66Q354 26 381 26 429 26 459 145 461 153 479 153H483Q499 153 499 144 499 139 496 130 455-11 378-11 333-11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206 200 217 182 220H180Q168 178 159 139T145 81 136 44 129 20 122 7 111-2Q98-11 83-11 66-11 57-1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640 121 647Z"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347 722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153 722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1299,0)"><path data-c="30" d="M96 585Q152 666 249 666 297 666 345 640T423 548Q460 465 460 320 460 165 417 83 397 41 362 16T301-15 250-22Q224-22 198-16T137 16 82 83Q39 165 39 320 39 494 96 585ZM321 597Q291 629 250 629 208 629 178 597 153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16 290 16 318 46 347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(0,1269.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341 56 388 89 425 135 442Q171 442 195 424T225 390 231 369Q231 367 232 367L243 378Q304 442 382 442 436 442 469 415T503 336 465 179 427 52Q427 26 444 26 450 26 453 27 482 32 505 65T540 145Q542 153 560 153 580 153 580 145 580 144 576 130 568 101 554 73T508 17 439-10Q392-10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 180T152 343Q153 348 153 366 153 405 129 405 91 405 66 305 60 285 60 284 58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406 158 442Q199 442 224 419T250 355Q248 336 247 334 247 331 231 288T198 191 182 105Q182 62 196 45T238 27Q261 27 281 38T312 61 339 94Q339 95 344 114T358 173 377 247Q415 397 419 404 432 431 462 431 475 431 483 424T494 412 496 403Q496 390 447 193T391-23Q363-106 294-155T156-205Q111-205 77-183T43-117Q43-95 50-80T69-58 89-48 106-45Q150-45 150-87 150-107 138-122T115-142 102-147L99-148Q101-153 118-160T152-167H160Q177-167 186-165 219-156 247-127T290-65 313-9 321 21L315 17Q309 13 296 6T270-6Q250-11 231-11 185-11 150 11T104 82Q103 89 103 113 103 170 138 262T173 379Q173 380 173 381 173 390 173 393T169 400 158 404H154Q131 404 112 385T82 344 65 302 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1029.5,0)"><path data-c="2212" d="M84 237T84 250 98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1807.5,0)"><path data-c="31" d="M213 578 200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641 273 663Q275 666 285 666 294 666 302 660V361L303 61Q310 54 315 52T339 48 401 46H427V0H416Q395 3 257 3 121 3 100 0H88V46H114Q136 46 152 46T177 47 193 50 201 52 207 57 213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(5643.6,0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638 32 641 30 647 33 664 42 682Q44 683 56 683 104 680 165 680 288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624 242 619 292 477T343 333L346 336Q350 340 358 349T379 373 411 410 454 461Q546 568 561 587T577 618Q577 634 545 637 528 637 528 647 528 649 530 661 533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673 763 669 760 657T755 643Q753 637 734 637 662 632 617 587 608 578 477 424L348 273 322 169Q295 62 295 57 295 46 363 46 379 46 384 45T390 35Q390 33 388 23 384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9 84 14 87 24 88 27 89 30T90 35 91 39 93 42 96 44 101 45 107 45 116 46 129 46Q168 47 180 50T198 63Q201 68 227 171L252 274 129 623Q128 624 127 625T125 627 122 629 118 631 113 633 105 634 96 635 83 636 66 637Z"></path></g><g data-mml-node="mi" transform="translate(6406.6,0)"><path data-c="1D45C" d="M201-11Q126-11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441 333 441 341 440 354 437 367 433T402 417 438 387 464 338 476 268Q476 161 390 75T201-11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375 346 405 306 405 243 405 195 347 158 303 140 230T121 120Z"></path></g><g data-mml-node="msubsup" transform="translate(6891.6,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685 294 674 258 534 220 386 220 383 220 381 227 388 288 442 357 442 411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145 555 144 551 130 535 71 500 33 466-10 419-10H414Q367-10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293 164 158Q132 28 127 16 114-11 83-11 69-11 59-2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(609,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g><g data-mml-node="mi" transform="translate(609,-317.1) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686 294 679 244 477 194 279 194 272 213 282 223 291 247 309 292 354T362 415Q402 442 438 442 468 442 485 423T503 369Q503 344 496 327T477 302 456 291 438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316 228 255Q230 254 243 252T267 246 293 238 320 224 342 206 359 180 365 147Q365 130 360 106T354 66Q354 26 381 26 429 26 459 145 461 153 479 153H483Q499 153 499 144 499 139 496 130 455-11 378-11 333-11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206 200 217 182 220H180Q168 178 159 139T145 81 136 44 129 20 122 7 111-2Q98-11 83-11 66-11 57-1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640 121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(8566.9,0)"><path data-c="2217" d="M229 286Q216 420 216 436 216 454 240 464 241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315 420 312 357 282T289 250L355 219 425 184Q434 175 434 161 434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63 283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127 106 124 100 124 87 124 76 134T64 161Q64 166 64 169T67 175 72 181 81 188 94 195 113 204 138 215 170 230 210 250L74 315Q65 324 65 338 65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(9289.1,0)"><path data-c="1D459" d="M117 59Q117 26 142 26 179 26 205 131 211 151 215 152 217 153 225 153H229Q238 153 241 153T246 151 248 144Q247 138 245 128T234 90 214 43 183 6 137-11Q101-11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623 167 626 166 628T162 632 157 634 149 635 141 636 132 637 122 637Q112 637 109 637T101 638 95 641 94 647Q94 649 96 661 101 680 107 682T179 688Q194 689 213 690T243 693 254 694Q266 694 266 686 266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(9587.1,0)"><path data-c="1D45C" d="M201-11Q126-11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441 333 441 341 440 354 437 367 433T402 417 438 387 464 338 476 268Q476 161 390 75T201-11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375 346 405 306 405 243 405 195 347 158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(10072.1,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412 431 419 447 422Q461 422 470 413T480 394Q480 379 423 152T363-80Q345-134 286-169T151-205Q10-205 10-137 10-111 28-91T74-71Q89-71 102-80T116-111Q116-121 114-130T107-144 99-154 92-162L90-164H91Q101-167 151-167 189-167 211-155 234-144 254-122T282-75Q288-56 298-13 311 35 311 43ZM384 328 380 339Q377 350 375 354T369 368 359 382 346 393 328 402 306 405Q262 405 221 352 191 313 171 233T151 117Q151 38 213 38 269 38 323 108L331 118 384 328Z"></path></g><g data-mml-node="mo" transform="translate(10549.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="msubsup" transform="translate(10938.1,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392 386 422 416 422 429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487Q506 153 506 144 506 138 501 117T481 63 449 13Q436 0 417-8 409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157ZM351 328Q351 334 346 350T323 385 277 405Q242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(562,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g><g data-mml-node="mi" transform="translate(562,-317.1) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686 294 679 244 477 194 279 194 272 213 282 223 291 247 309 292 354T362 415Q402 442 438 442 468 442 485 423T503 369Q503 344 496 327T477 302 456 291 438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316 228 255Q230 254 243 252T267 246 293 238 320 224 342 206 359 180 365 147Q365 130 360 106T354 66Q354 26 381 26 429 26 459 145 461 153 479 153H483Q499 153 499 144 499 139 496 130 455-11 378-11 333-11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206 200 217 182 220H180Q168 178 159 139T145 81 136 44 129 20 122 7 111-2Q98-11 83-11 66-11 57-1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640 121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(12344.2,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p><p>其中<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="4.127ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1824 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638 32 641 30 647 33 664 42 682Q44 683 56 683 104 680 165 680 288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624 242 619 292 477T343 333L346 336Q350 340 358 349T379 373 411 410 454 461Q546 568 561 587T577 618Q577 634 545 637 528 637 528 647 528 649 530 661 533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673 763 669 760 657T755 643Q753 637 734 637 662 632 617 587 608 578 477 424L348 273 322 169Q295 62 295 57 295 46 363 46 379 46 384 45T390 35Q390 33 388 23 384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9 84 14 87 24 88 27 89 30T90 35 91 39 93 42 96 44 101 45 107 45 116 46 129 46Q168 47 180 50T198 63Q201 68 227 171L252 274 129 623Q128 624 127 625T125 627 122 629 118 631 113 633 105 634 96 635 83 636 66 637Z"></path></g><g data-mml-node="mi" transform="translate(763,0)"><path data-c="1D45C" d="M201-11Q126-11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441 333 441 341 440 354 437 367 433T402 417 438 387 464 338 476 268Q476 161 390 75T201-11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375 346 405 306 405 243 405 195 347 158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1248,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685 294 674 258 534 220 386 220 383 220 381 227 388 288 442 357 442 411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145 555 144 551 130 535 71 500 33 466-10 419-10H414Q367-10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293 164 158Q132 28 127 16 114-11 83-11 69-11 59-2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container></span>(Y one hot) 是输出的独热编码。最后模型在训练集和测试集上的准确率能够达到 97% 和 86% ，同时对于一些在训练集中没有出现过的单词 (例如: adore) 也能得到不错的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_my_sentences = np.array([<span class="string">"i adore you"</span>, <span class="string">"i love you"</span>, <span class="string">"funny lol"</span>, <span class="string">"lets play with a ball"</span>, <span class="string">"food is ready"</span>, <span class="string">"you are not happy"</span>])</span><br><span class="line">Y_my_labels = np.array([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">2</span>], [<span class="number">1</span>], [<span class="number">4</span>],[<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)</span><br><span class="line">print_predictions(X_my_sentences, pred)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.8333333333333334</span><br><span class="line"></span><br><span class="line">i adore you ❤️</span><br><span class="line">i love you ❤️</span><br><span class="line">funny lol 😄</span><br><span class="line">lets play with a ball ⚾</span><br><span class="line">food is ready 🍴</span><br><span class="line">you are not happy ❤️</span><br></pre></td></tr></table></figure><p>但是该模型并不能分析 not happy 是表示不开心，而只是简单地学习了 happy 这个单词。输出模型的混淆矩阵看一下模型的表现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Y_test.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'           '</span>+ label_to_emoji(<span class="number">0</span>)+ <span class="string">'    '</span> + label_to_emoji(<span class="number">1</span>) + <span class="string">'    '</span> +  label_to_emoji(<span class="number">2</span>)+ <span class="string">'    '</span> + label_to_emoji(<span class="number">3</span>)+<span class="string">'   '</span> + label_to_emoji(<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(pd.crosstab(Y_test, pred_test.reshape(<span class="number">56</span>,), rownames=[<span class="string">'Actual'</span>], colnames=[<span class="string">'Predicted'</span>], margins=<span class="literal">True</span>))</span><br><span class="line">plot_confusion_matrix(Y_test, pred_test)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(56,)</span><br><span class="line">           ❤️   ⚾   😄   😞  🍴</span><br><span class="line">Predicted  0.0  1.0  2.0  3.0  4.0  All</span><br><span class="line">Actual                                 </span><br><span class="line">0            6    0    0    1    0    7</span><br><span class="line">1            0    8    0    0    0    8</span><br><span class="line">2            2    0   16    0    0   18</span><br><span class="line">3            1    1    2   12    0   16</span><br><span class="line">4            0    0    1    0    6    7</span><br><span class="line">All          9    9   19   13    6   56</span><br></pre></td></tr></table></figure><p><img src="output.png"></p><p>矩阵对角线上的颜色比较深，表示模型的表现还不错。但是模型却无法分析 not xxx 这类的短语，因为嵌入矩阵中没有对应的表示，而且单纯地对所有单词的嵌入求平均会丢失输入的单词的顺序，因此需要更好的算法。</p><h2 id="emojifier-v2-在-keras-中使用-lstms">Emojifier-V2: 在 Keras 中使用 LSTMs</h2><p>Emojifier-V2 的概况如下图所示：</p><p><img src="emojifier-v2.png"></p><p>这是一个两层的 LSTM 序列分类器。这次实验使用 mini-batches 来训练 Keras，因此一个 batch 中的序列的长度应该相同，因此需要补 0。例如一个 batch 中的序列的最大长度为 5，那么 "I love you" 这个句子的表示为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.667ex" xmlns="http://www.w3.org/2000/svg" width="17.574ex" height="3.086ex" role="img" focusable="false" viewBox="0 -1069 7767.6 1364"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350 174 402 244 433 307 442H310Q355 442 388 420T421 355Q421 265 310 237 261 224 176 223 139 223 138 221 138 219 132 186T125 128Q125 81 146 54T209 26 302 45 394 111Q403 121 406 121 410 121 419 112T429 98 420 82 390 55 344 24 281-1 205-11Q126-11 83 42T39 168ZM373 353Q367 405 305 405 272 405 244 391T199 357 170 316 154 280 149 261Q149 260 169 260 282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(499,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1182,0)"><path data-c="2C" d="M78 35T78 60 94 103 137 121Q165 121 187 96T210 8Q210-27 201-60T180-117 154-158 130-185 117-194Q113-194 104-185T95-172Q95-168 106-156T131-126 157-76 173-3V9L172 8Q170 7 167 6T161 3 152 1 140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1626.6,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350 174 402 244 433 307 442H310Q355 442 388 420T421 355Q421 265 310 237 261 224 176 223 139 223 138 221 138 219 132 186T125 128Q125 81 146 54T209 26 302 45 394 111Q403 121 406 121 410 121 419 112T429 98 420 82 390 55 344 24 281-1 205-11Q126-11 83 42T39 168ZM373 353Q367 405 305 405 272 405 244 391T199 357 170 316 154 280 149 261Q149 260 169 260 282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(499,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26 179 26 205 131 211 151 215 152 217 153 225 153H229Q238 153 241 153T246 151 248 144Q247 138 245 128T234 90 214 43 183 6 137-11Q101-11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623 167 626 166 628T162 632 157 634 149 635 141 636 132 637 122 637Q112 637 109 637T101 638 95 641 94 647Q94 649 96 661 101 680 107 682T179 688Q194 689 213 690T243 693 254 694Q266 694 266 686 266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298,0)"><path data-c="1D45C" d="M201-11Q126-11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441 333 441 341 440 354 437 367 433T402 417 438 387 464 338 476 268Q476 161 390 75T201-11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375 346 405 306 405 243 405 195 347 158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(783,0)"><path data-c="1D463" d="M173 380Q173 405 154 405 130 405 104 376T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287 21 294 29 316T53 368 97 419 160 441Q202 441 225 417T249 361Q249 344 246 335 246 329 231 291T200 202 182 113Q182 86 187 69 200 26 250 26 287 26 319 60T369 139 398 222 409 277Q409 300 401 317T383 343 365 361 357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159 347 40 241-11Q177-11 139 22 102 54 102 117 102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(1268,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350 174 402 244 433 307 442H310Q355 442 388 420T421 355Q421 265 310 237 261 224 176 223 139 223 138 221 138 219 132 186T125 128Q125 81 146 54T209 26 302 45 394 111Q403 121 406 121 410 121 419 112T429 98 420 82 390 55 344 24 281-1 205-11Q126-11 83 42T39 168ZM373 353Q367 405 305 405 272 405 244 391T199 357 170 316 154 280 149 261Q149 260 169 260 282 260 327 284T373 353Z"></path></g></g></g><g data-mml-node="mo" transform="translate(3401.7,0)"><path data-c="2C" d="M78 35T78 60 94 103 137 121Q165 121 187 96T210 8Q210-27 201-60T180-117 154-158 130-185 117-194Q113-194 104-185T95-172Q95-168 106-156T131-126 157-76 173-3V9L172 8Q170 7 167 6T161 3 152 1 140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3846.4,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350 174 402 244 433 307 442H310Q355 442 388 420T421 355Q421 265 310 237 261 224 176 223 139 223 138 221 138 219 132 186T125 128Q125 81 146 54T209 26 302 45 394 111Q403 121 406 121 410 121 419 112T429 98 420 82 390 55 344 24 281-1 205-11Q126-11 83 42T39 168ZM373 353Q367 405 305 405 272 405 244 391T199 357 170 316 154 280 149 261Q149 260 169 260 282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(499,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406 158 442Q199 442 224 419T250 355Q248 336 247 334 247 331 231 288T198 191 182 105Q182 62 196 45T238 27Q261 27 281 38T312 61 339 94Q339 95 344 114T358 173 377 247Q415 397 419 404 432 431 462 431 475 431 483 424T494 412 496 403Q496 390 447 193T391-23Q363-106 294-155T156-205Q111-205 77-183T43-117Q43-95 50-80T69-58 89-48 106-45Q150-45 150-87 150-107 138-122T115-142 102-147L99-148Q101-153 118-160T152-167H160Q177-167 186-165 219-156 247-127T290-65 313-9 321 21L315 17Q309 13 296 6T270-6Q250-11 231-11 185-11 150 11T104 82Q103 89 103 113 103 170 138 262T173 379Q173 380 173 381 173 390 173 393T169 400 158 404H154Q131 404 112 385T82 344 65 302 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(490,0)"><path data-c="1D45C" d="M201-11Q126-11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441 333 441 341 440 354 437 367 433T402 417 438 387 464 338 476 268Q476 161 390 75T201-11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375 346 405 306 405 243 405 195 347 158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(975,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370 99 420 158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27 291 44 328 78L339 95Q341 99 377 247 407 367 413 387T427 416Q444 431 463 431 480 431 488 421T496 402L420 84Q419 79 419 68 419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153 551 153 551 144 550 139 549 130T540 98 523 55 498 17 462-8Q454-10 438-10 372-10 347 46 345 45 336 36T318 21 296 6 267-6 233-11Q189-11 155 7 103 38 103 113 103 170 138 262T173 379Q173 380 173 381 173 390 173 393T169 400 158 404H154Q131 404 112 385T82 344 65 302 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5489.3,0)"><path data-c="2C" d="M78 35T78 60 94 103 137 121Q165 121 187 96T210 8Q210-27 201-60T180-117 154-158 130-185 117-194Q113-194 104-185T95-172Q95-168 106-156T131-126 157-76 173-3V9L172 8Q170 7 167 6T161 3 152 1 140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5934,0)"><g data-mml-node="mover"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666 297 666 345 640T423 548Q460 465 460 320 460 165 417 83 397 41 362 16T301-15 250-22Q224-22 198-16T137 16 82 83Q39 165 39 320 39 494 96 585ZM321 597Q291 629 250 629 208 629 178 597 153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16 290 16 318 46 347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(250,255) translate(-250 0)"><path data-c="20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709 414 705 419 690 429 653 460 633 471 626 471 615 471 606 468 603T454 594Q411 572 379 531 377 529 374 525T369 519 364 517 357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615 29 622 42 635H401Q377 673 377 694Z"></path></g></g></g><g data-mml-node="mo" transform="translate(6434,0)"><path data-c="2C" d="M78 35T78 60 94 103 137 121Q165 121 187 96T210 8Q210-27 201-60T180-117 154-158 130-185 117-194Q113-194 104-185T95-172Q95-168 106-156T131-126 157-76 173-3V9L172 8Q170 7 167 6T161 3 152 1 140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6878.6,0)"><g data-mml-node="mover"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666 297 666 345 640T423 548Q460 465 460 320 460 165 417 83 397 41 362 16T301-15 250-22Q224-22 198-16T137 16 82 83Q39 165 39 320 39 494 96 585ZM321 597Q291 629 250 629 208 629 178 597 153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16 290 16 318 46 347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(250,255) translate(-250 0)"><path data-c="20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709 414 705 419 690 429 653 460 633 471 626 471 615 471 606 468 603T454 594Q411 572 379 531 377 529 374 525T369 519 364 517 357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615 29 622 42 635H401Q377 673 377 694Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7378.6,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>。</p><h3 id="embedding-层">Embedding 层</h3><p>在 Keras 中，嵌入矩阵被表示成一个层，然后将词的索引匹配成嵌入向量。嵌入矩阵可以被训练出来，也可以用一个训练好的矩阵来初始化它。<code>Embedding()</code> 层输出是一个 (batch size, max input length, dimension of word vectors) 的矩阵。word_to_index 的实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sentences_to_indices</span>(<span class="params">X, word_to_index, max_len</span>):</span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape</span></span><br><span class="line">    X_indices = np.zeros((m, max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] = word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure><p>接下来需要实现预训练的 Embedding 层，将训练好的嵌入矩阵设置到 <code>Embedding()</code> 层的权值中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pretrained_embedding_layer</span>(<span class="params">word_to_vec_map, word_to_index</span>):</span><br><span class="line">    vocab_len = <span class="built_in">len</span>(word_to_index) + <span class="number">1</span>                  <span class="comment"># adding 1 to fit Keras embedding (requirement)</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable.</span></span><br><span class="line">    <span class="comment"># Use Embedding(...). Make sure to set trainable=False.</span></span><br><span class="line">    embedding_layer = Embedding(vocab_len, emb_dim, trainable = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="literal">None</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure><h3 id="构建模型">构建模型</h3><p>接下来需要构建模型，模型分为：</p><ul><li>输入层: <code>Input((max_len, m), dtype='int32')</code></li><li>LSTM 层: <code>LSTM(hidden_units, return_sequence)(embeddings)</code></li><li>Dropout 层: <code>Dropout(keep_prob)(X)</code></li><li>全连接层: <code>Dense(output_dimension)(X)</code></li><li>激活层: <code>Activation(activation_func)(X)</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Emojify_V2</span>(<span class="params">input_shape, word_to_vec_map, word_to_index</span>):</span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    sentence_indices = Input(input_shape, dtype=<span class="string">'int32'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    embeddings = embedding_layer(sentence_indices)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    X = LSTM(<span class="number">128</span>, return_sequences=<span class="literal">True</span>)(embeddings)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    X = LSTM(<span class="number">128</span>, return_sequences=<span class="literal">False</span>)(X)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    X = Dense(<span class="number">5</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    X = Activation(<span class="string">'softmax'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    model = Model(inputs=sentence_indices, outputs=X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>构建好模型后可以通过模型的 <code>summary()</code> 方法来检查模型的概要 (max_len = 10)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         (None, 10)                0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">embedding_2 (Embedding)      (None, 10, 50)            20000050  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_1 (LSTM)                (None, 10, 128)           91648     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)          (None, 10, 128)           0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_2 (LSTM)                (None, 128)               131584    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)          (None, 128)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 5)                 645       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">activation_1 (Activation)    (None, 5)                 0         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 20,223,927</span><br><span class="line">Trainable params: 223,877</span><br><span class="line">Non-trainable params: 20,000,050</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>由于嵌入矩阵是训练好的 <code>trainable = False</code>，因此有 400,001 * 50 = 20,000,050 个参数是 Non-trainable 参数。接下来需要编译模型，定义损失函数、优化器和评估指标，最后拟合模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)</span><br><span class="line">Y_train_oh = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br><span class="line">model.fit(X_train_indices, Y_train_oh, epochs = <span class="number">50</span>, batch_size = <span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>训练集和测试集上的准确率能接近 100% 和 91%。对于 not happy 也能准确预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_test = np.array([<span class="string">'you are not happy'</span>])</span><br><span class="line">X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)</span><br><span class="line"><span class="built_in">print</span>(x_test[<span class="number">0</span>] +<span class="string">' '</span>+  label_to_emoji(np.argmax(model.predict(X_test_indices))))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">you are not happy 😞</span><br></pre></td></tr></table></figure><p>因为 LSTM 网络具有长短期记忆，所以能够很好地预测某些单词的组合。</p><h2 id="总结">总结</h2><p>在 NLP 任务中，如果训练集比较小，比较适合直接用训练好的嵌入矩阵而不是自己训练一个。在 RNN 中，如果想用 mini-batches 提高效率(矩阵的运算比循环快)，那么就需要对样本进行补 0。<code>LSTM()</code> 的 <code>return_sequence</code> 参数决定返回所有的隐藏状态还是只返回最后一个时间步的隐藏状态。</p><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"><img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://pengzhendong.github.io/2018/08/31/emojify/" title="Emojify 文本情感分析">https://pengzhendong.github.io/2018/08/31/emojify/</a></li><li class="post-copyright-license"><strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/08/28/jazz-with-an-lstm-network/" rel="prev" title="用 LSTM 网络创作爵士独奏"><i class="fa fa-chevron-left"></i> 用 LSTM 网络创作爵士独奏</a></div><div class="post-nav-item"><a href="/2018/08/31/word-vector-representation/" rel="next" title="词向量表示">词向量表示<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener("tabs:register",()=>{let{activeClass:t}=CONFIG.comments;if(CONFIG.comments.storage&&(t=localStorage.getItem("comments_active")||t),t){let e=document.querySelector(`a[href="#comment-${t}"]`);e&&e.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{if(!t.target.matches(".tabs-comment .tab-content .tab-pane"))return;let e=t.target.classList[1];localStorage.setItem("comments_active",e)})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#baseline-%E6%A8%A1%E5%9E%8B-emojifier-v1"><span class="nav-number">2.</span> <span class="nav-text">Baseline 模型: Emojifier-V1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#emojifier-v1-%E6%A6%82%E5%86%B5"><span class="nav-number">2.1.</span> <span class="nav-text">Emojifier-V1 概况</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-emojifier-v1"><span class="nav-number">2.2.</span> <span class="nav-text">实现 Emojifier-V1</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#emojifier-v2-%E5%9C%A8-keras-%E4%B8%AD%E4%BD%BF%E7%94%A8-lstms"><span class="nav-number">3.</span> <span class="nav-text">Emojifier-V2: 在 Keras 中使用 LSTMs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-%E5%B1%82"><span class="nav-number">3.1.</span> <span class="nav-text">Embedding 层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.</span> <span class="nav-text">构建模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">5.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">35</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https:&#x2F;&#x2F;www.facebook.com&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https:&#x2F;&#x2F;t.me&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https:&#x2F;&#x2F;weibo.com&#x2F;qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" title="关于 → &#x2F;about"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"><img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">230k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:29</span></div><script>!function(){function e(e){return e=encodeURI(e),document.getElementById(e).querySelector(".leancloud-visitors-count")}let{app_id:t,app_key:o,server_url:n}={enable:!0,app_id:"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz",app_key:"JRfKfM8mRPgxMB9GOSAnix9W",server_url:null,security:!0};function r(n){var r=(e,r,l)=>fetch(`${n}/1.1${r}`,{method:e,headers:{"X-LC-Id":t,"X-LC-Key":o,"Content-Type":"application/json"},body:JSON.stringify(l)});if(CONFIG.page.isPost){if(CONFIG.hostname!==location.hostname)return;!function(t){var o=document.querySelector(".leancloud_visitors"),n=decodeURI(o.id);o.dataset.flagTitle,t("get","/classes/Counter?where="+encodeURIComponent(JSON.stringify({url:n}))).then(e=>e.json()).then(({results:o})=>{if(o.length>0){var r=o[0];e(n).innerText=r.time+1,t("put","/classes/Counter/"+r.objectId,{time:{__op:"Increment",amount:1}}).catch(e=>{console.error("Failed to save visitor count",e)})}else e(n).innerText="Counter not initialized! More info at console err msg.",console.error("ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.")}).catch(e=>{console.error("LeanCloud Counter Error",e)})}(r)}else document.querySelectorAll(".post-title-link").length>=1&&function(t){var o=[...document.querySelectorAll(".leancloud_visitors")].map(e=>decodeURI(e.id));t("get","/classes/Counter?where="+encodeURIComponent(JSON.stringify({url:{$in:o}}))).then(e=>e.json()).then(({results:t})=>{for(let n of o){let o=t.find(e=>e.url===n);e(n).innerText=o?o.time:0}}).catch(e=>{console.error("LeanCloud Counter Error",e)})}(r)}let l="-MdYXbMMI"!==t.slice(-9)?n:`https://${t.slice(0,8).toLowerCase()}.api.lncldglobal.com`;l?r(l):fetch("https://app-router.leancloud.cn/2/route?appId="+t).then(e=>e.json()).then(({api_server:e})=>{r("https://"+e)})}()</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var e,t,o=document.getElementsByTagName("link");if(o.length>0)for(i=0;i<o.length;i++)"canonical"==o[i].rel.toLowerCase()&&o[i].href&&(e=o[i].href);t=e?e.split(":")[0]:window.location.protocol.split(":")[0],e||(e=window.location.href),function(){var i=e,o=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(i)){var n="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),i&&(n+="&l="+i)):i&&(n+="?l="+i),(new Image).src=n}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams"},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const a=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],a),d=document.createTextNode("");t.parentNode.replaceChild(d,t),n.start={node:d,delim:"",n:0},n.end={node:d,delim:"",n:0},e.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script></body></html>