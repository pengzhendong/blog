<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!0,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"0C48U14D5U",apiKey:"796da148bd60fa95ebae037cae6d5c16",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 在机器学习中经常使用梯度下降算法来优化代价函数，得到局部最优解。但是梯度下降算法有时候效率并不高，有一些算法能够很大程度上提高梯度下降算法的性能。例如前面提到的小批量梯度下降，每次使用一部分样本更新参数，能够加速训练过程，还有特征缩放。"><meta property="og:type" content="article"><meta property="og:title" content="特征缩放"><meta property="og:url" content="https://pengzhendong.github.io/2018/05/31/feature-scaling/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 在机器学习中经常使用梯度下降算法来优化代价函数，得到局部最优解。但是梯度下降算法有时候效率并不高，有一些算法能够很大程度上提高梯度下降算法的性能。例如前面提到的小批量梯度下降，每次使用一部分样本更新参数，能够加速训练过程，还有特征缩放。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2018-05-31T21:04:59.000Z"><meta property="article:modified_time" content="2018-05-31T23:13:40.000Z"><meta property="article:author" content="Randy Peng"><meta property="article:tag" content="Machine Learning"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://pengzhendong.github.io/2018/05/31/feature-scaling/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>特征缩放 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.github.io/2018/05/31/feature-scaling/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">特征缩放</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-05-31 21:04:59 / 修改时间：23:13:40" itemprop="dateCreated datePublished" datetime="2018-05-31T21:04:59+00:00">2018-05-31</time></span><span id="/2018/05/31/feature-scaling/" class="post-meta-item leancloud_visitors" data-flag-title="特征缩放" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>1.8k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>2 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>在机器学习中经常使用梯度下降算法来优化代价函数，得到局部最优解。但是梯度下降算法有时候效率并不高，有一些算法能够很大程度上提高梯度下降算法的性能。例如前面提到的小批量梯度下降，每次使用一部分样本更新参数，能够加速训练过程，还有<strong>特征缩放</strong>。</p><span id="more"></span><h2 id="特征缩放">特征缩放</h2><p>在实际应用中，我们得到的数据会存在缺失值、重复值等各种问题，所以数据的预处理就显得尤为重要。特征缩放是一种常用的数据处理方式，使用特征缩放能够加快梯度下降，提高收敛速度。Normalization 这个词翻译成归一化不太好理解，网上的各种资料更是滥用归一化和标准化 (Standardization)，下面只能结合着 <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/preprocessing.html#">sklearn</a> 的官方文档给出自己的理解。</p><p>人们常说的归一化其实就是普通的特征缩放(scaling)，通过线性变换对数据进行缩放，简化计算的方式，将有量纲的输入，变换为无量纲。例如：房价预测问题中房间的大小 (30~100<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="2.974ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 1314.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341 56 388 88 425 132 442 175 435 205 417 221 395 229 376L231 369Q231 367 232 367L243 378Q303 442 384 442 401 442 415 440T441 433 460 423 475 411 485 398 493 385 497 373 500 364 502 357L510 367Q573 442 659 442 713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145 857 144 853 130 845 101 831 73T785 17 716-10Q669-10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291 466 157Q433 26 428 16 415-11 385-11 372-11 364-4T353 8 350 18Q350 29 384 161L420 307Q423 322 423 345 423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 181 151 335 151 342 154 357 154 369 154 405 129 405 107 405 92 377T69 316 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(911,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315 301 241Q265 210 201 149L142 93 218 92Q375 92 385 97 392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19 31Q50 38 56 46T86 81Q115 113 136 137 145 147 170 174T204 211 233 244 261 278 284 308 305 340 320 369 333 401 340 431 343 464Q343 527 309 573T212 619Q179 619 154 602T119 569 109 550Q109 549 114 549 132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container></span>) 和房间数 (3~5)，不同量纲的特征会导致在梯度下降时”步伐“ (<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="4.244ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1876 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374 564 388 566 390T582 393Q603 393 603 385 603 376 594 346T558 261 497 161L486 147 487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62 597 56 591 43 579 19 556 5T512-10H505Q438-10 414 62L411 69 400 61Q390 53 370 41T325 18 267-2 203-11Q124-11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290 382 405 304 405 235 405 183 332 156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g><g data-mml-node="mi" transform="translate(640,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686 523 679 450 384T375 83 374 68Q374 26 402 26 411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487 491Q506 153 506 145 506 140 503 129 490 79 473 48T445 8 417-8Q409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400 369 394 369 396 370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405 242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109 352 326Z"></path></g><g data-mml-node="mi" transform="translate(1160,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253 656 197 644 161T609 80 554 12 482-11Q438-11 404 5T355 48Q354 47 352 44 311-11 252-11 226-11 202-5T155 14 118 53 104 116Q104 170 138 262T173 379Q173 380 173 381 173 390 173 393T169 400 158 404H154Q131 404 112 385T82 344 65 302 57 280Q55 278 41 278H27Q21 284 21 287 21 293 29 315T52 366 96 418 161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136 341 143 342 152 345 165 348 182 354 206 362 238 373 281Q402 395 406 404 419 431 449 431 468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42 487 26Q558 26 600 148 609 171 620 213T632 273Q632 306 619 325T593 357 580 385Z"></path></g></g></g></svg></mjx-container></span>) 不同，学习率太小收敛慢，学习率太大，有些特征(例如房间大小)的权值甚至可能不会收敛；在使用 Sigmoid 或者 Tanh 作为激活函数时也容易出现饱和现象。(详情参考：<a target="_blank" rel="noopener" href="https://www.robertoreif.com/blog/2017/12/21/importance-of-feature-scaling-in-data-modeling-part-2">Importance of Feature Scaling in Data Modeling (Part 2)</a>)</p><p>归一化有不同的策略，常用的归一化方法有以下几种：</p><ul><li>Mean Normalization</li></ul><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align:-1.602ex" xmlns="http://www.w3.org/2000/svg" width="20.66ex" height="4.905ex" role="img" focusable="false" viewBox="0 -1460 9131.7 2168"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(936.2,413) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560 218 560 240 545T262 501Q262 496 260 486 259 479 173 263T84 45 79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(1458.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347 722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153 722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2514.2,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1074.2,0)"><path data-c="2212" d="M84 237T84 250 98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2074.4,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341 56 388 88 425 132 442 175 435 205 417 221 395 229 376L231 369Q231 367 232 367L243 378Q303 442 384 442 401 442 415 440T441 433 460 423 475 411 485 398 493 385 497 373 500 364 502 357L510 367Q573 442 659 442 713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145 857 144 853 130 845 101 831 73T785 17 716-10Q669-10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291 466 157Q433 26 428 16 415-11 385-11 372-11 364-4T353 8 350 18Q350 29 384 161L420 307Q423 322 423 345 423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 181 151 335 151 342 154 357 154 369 154 405 129 405 107 405 92 377T69 316 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2952.4,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350 174 402 244 433 307 442H310Q355 442 388 420T421 355Q421 265 310 237 261 224 176 223 139 223 138 221 138 219 132 186T125 128Q125 81 146 54T209 26 302 45 394 111Q403 121 406 121 410 121 419 112T429 98 420 82 390 55 344 24 281-1 205-11Q126-11 83 42T39 168ZM373 353Q367 405 305 405 272 405 244 391T199 357 170 316 154 280 149 261Q149 260 169 260 282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(3418.4,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392 386 422 416 422 429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487Q506 153 506 144 506 138 501 117T481 63 449 13Q436 0 417-8 409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157ZM351 328Q351 334 346 350T323 385 277 405Q242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(3947.4,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341 56 388 89 425 135 442Q171 442 195 424T225 390 231 369Q231 367 232 367L243 378Q304 442 382 442 436 442 469 415T503 336 465 179 427 52Q427 26 444 26 450 26 453 27 482 32 505 65T540 145Q542 153 560 153 580 153 580 145 580 144 576 130 568 101 554 73T508 17 439-10Q392-10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 180T152 343Q153 348 153 366 153 405 129 405 91 405 66 305 60 285 60 284 58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4547.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4936.4,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(5788.4,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g><g data-mml-node="mi" transform="translate(2986.2,-686)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284 308 311 278 321T236 341Q176 383 176 462 176 523 208 573T273 648Q302 673 343 688T407 704H418 425Q521 704 564 640 565 640 577 653T603 682 623 704Q624 704 627 704T632 705Q645 705 645 698T617 577 585 459 569 456Q549 456 549 465 549 471 550 475 550 478 551 494T553 520Q553 554 544 579T526 616 501 641Q465 662 419 662 362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186 541 164 528 137T492 78 426 18 332-20Q320-22 298-22 199-22 144 33L134 44 106 13Q83-14 78-18T65-22Q52-22 52-14 52-11 110 221 112 227 130 227H143Q149 221 149 216 149 214 148 207T144 186 142 153Q144 114 160 87T203 47 255 29 308 24Z"></path></g><rect width="6377.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p><p>Mean Normalization减去均值将数据中心化 (0 均值化)，再除以<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.05ex" xmlns="http://www.w3.org/2000/svg" width="1.459ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 645 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284 308 311 278 321T236 341Q176 383 176 462 176 523 208 573T273 648Q302 673 343 688T407 704H418 425Q521 704 564 640 565 640 577 653T603 682 623 704Q624 704 627 704T632 705Q645 705 645 698T617 577 585 459 569 456Q549 456 549 465 549 471 550 475 550 478 551 494T553 520Q553 554 544 579T526 616 501 641Q465 662 419 662 362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186 541 164 528 137T492 78 426 18 332-20Q320-22 298-22 199-22 144 33L134 44 106 13Q83-14 78-18T65-22Q52-22 52-14 52-11 110 221 112 227 130 227H143Q149 221 149 216 149 214 148 207T144 186 142 153Q144 114 160 87T203 47 255 29 308 24Z"></path></g></g></g></svg></mjx-container></span>进行缩放。<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.05ex" xmlns="http://www.w3.org/2000/svg" width="1.459ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 645 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284 308 311 278 321T236 341Q176 383 176 462 176 523 208 573T273 648Q302 673 343 688T407 704H418 425Q521 704 564 640 565 640 577 653T603 682 623 704Q624 704 627 704T632 705Q645 705 645 698T617 577 585 459 569 456Q549 456 549 465 549 471 550 475 550 478 551 494T553 520Q553 554 544 579T526 616 501 641Q465 662 419 662 362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186 541 164 528 137T492 78 426 18 332-20Q320-22 298-22 199-22 144 33L134 44 106 13Q83-14 78-18T65-22Q52-22 52-14 52-11 110 221 112 227 130 227H143Q149 221 149 216 149 214 148 207T144 186 142 153Q144 114 160 87T203 47 255 29 308 24Z"></path></g></g></g></svg></mjx-container></span>可以取<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.566ex" xmlns="http://www.w3.org/2000/svg" width="18.743ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 8284.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341 56 388 88 425 132 442 175 435 205 417 221 395 229 376L231 369Q231 367 232 367L243 378Q303 442 384 442 401 442 415 440T441 433 460 423 475 411 485 398 493 385 497 373 500 364 502 357L510 367Q573 442 659 442 713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145 857 144 853 130 845 101 831 73T785 17 716-10Q669-10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291 466 157Q433 26 428 16 415-11 385-11 372-11 364-4T353 8 350 18Q350 29 384 161L420 307Q423 322 423 345 423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 181 151 335 151 342 154 357 154 369 154 405 129 405 107 405 92 377T69 316 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392 386 422 416 422 429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487Q506 153 506 144 506 138 501 117T481 63 449 13Q436 0 417-8 409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157ZM351 328Q351 334 346 350T323 385 277 405Q242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442 467 442 494 420T522 361Q522 332 508 314T481 292 458 288Q439 288 427 299T415 328Q415 374 465 391 454 404 425 404 412 404 406 402 368 386 350 336 290 115 290 78 290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145 504 144 502 134 486 77 440 33T333-11Q263-11 227 52 186-10 133-10H127Q78-10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101 142 81 130 66T107 46 94 41L91 40Q91 39 97 36T113 29 132 26Q168 26 194 71 203 87 217 139T245 247 261 313Q266 340 266 352 266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1979,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2368,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(3220,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3831.2,0)"><path data-c="2212" d="M84 237T84 250 98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(4831.4,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341 56 388 88 425 132 442 175 435 205 417 221 395 229 376L231 369Q231 367 232 367L243 378Q303 442 384 442 401 442 415 440T441 433 460 423 475 411 485 398 493 385 497 373 500 364 502 357L510 367Q573 442 659 442 713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145 857 144 853 130 845 101 831 73T785 17 716-10Q669-10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291 466 157Q433 26 428 16 415-11 385-11 372-11 364-4T353 8 350 18Q350 29 384 161L420 307Q423 322 423 345 423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 181 151 335 151 342 154 357 154 369 154 405 129 405 107 405 92 377T69 316 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5709.4,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6054.4,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341 56 388 89 425 135 442Q171 442 195 424T225 390 231 369Q231 367 232 367L243 378Q304 442 382 442 436 442 469 415T503 336 465 179 427 52Q427 26 444 26 450 26 453 27 482 32 505 65T540 145Q542 153 560 153 580 153 580 145 580 144 576 130 568 101 554 73T508 17 439-10Q392-10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 180T152 343Q153 348 153 366 153 405 129 405 91 405 66 305 60 285 60 284 58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(6654.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7043.4,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(7895.4,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>，或者取标准差<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.566ex" xmlns="http://www.w3.org/2000/svg" width="6.742ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2980 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415 300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372 367 378Q368 378 368 379 368 382 361 388T336 399 297 405Q249 405 227 379T204 326Q204 301 223 291T278 274 330 259Q396 230 396 163 396 135 385 107T352 51 289 7 195-10Q118-10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34 201 27Q237 27 263 38T301 66 318 97 323 122Q323 150 302 164T254 181 195 196 148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469,0)"><path data-c="1D461" d="M26 385Q19 392 19 395 19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566 179 586 187 603 197 615 211 624 229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420 330 398 317 385H210L174 240Q135 80 135 68 135 26 162 26 197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145 322 142 319 133 314 117 301 95T267 48 216 6 155-11Q125-11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383 128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(830,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686 523 679 450 384T375 83 374 68Q374 26 402 26 411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487 491Q506 153 506 145 506 140 503 129 490 79 473 48T445 8 417-8Q409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400 369 394 369 396 370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405 242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109 352 326Z"></path></g><g data-mml-node="mo" transform="translate(1350,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1739,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2591,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>，这时也叫做 Z-score 归一化或者标准化 (Standardization)。使用 <code>sklearn</code> 实现标准化如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scaler = sklearn.preprocessing.StandardScaler().fit(x_train)</span><br><span class="line">x_train = scaler.transform(x_train)</span><br><span class="line">x_test = scaler.transform(x_test)</span><br></pre></td></tr></table></figure><p>或者可以直接对数据集使用 <code>x_train = preprocessing.scale(X_train)</code> 进行缩放，使用 <code>StandardScaler</code> 类是为了让测试集和训练集进行同样的缩放，缩放后的数据具有零均值和标准方差。</p><ul><li>Min-max Normalization</li></ul><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align:-2.172ex" xmlns="http://www.w3.org/2000/svg" width="25.427ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 11238.7 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(936.2,413) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560 218 560 240 545T262 501Q262 496 260 486 259 479 173 263T84 45 79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(1458.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347 722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153 722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2514.2,0)"><g data-mml-node="mrow" transform="translate(1598.5,710)"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1074.2,0)"><path data-c="2212" d="M84 237T84 250 98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2074.4,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341 56 388 88 425 132 442 175 435 205 417 221 395 229 376L231 369Q231 367 232 367L243 378Q303 442 384 442 401 442 415 440T441 433 460 423 475 411 485 398 493 385 497 373 500 364 502 357L510 367Q573 442 659 442 713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145 857 144 853 130 845 101 831 73T785 17 716-10Q669-10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291 466 157Q433 26 428 16 415-11 385-11 372-11 364-4T353 8 350 18Q350 29 384 161L420 307Q423 322 423 345 423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 181 151 335 151 342 154 357 154 369 154 405 129 405 107 405 92 377T69 316 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2952.4,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3297.4,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341 56 388 89 425 135 442Q171 442 195 424T225 390 231 369Q231 367 232 367L243 378Q304 442 382 442 436 442 469 415T503 336 465 179 427 52Q427 26 444 26 450 26 453 27 482 32 505 65T540 145Q542 153 560 153 580 153 580 145 580 144 576 130 568 101 554 73T508 17 439-10Q392-10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 180T152 343Q153 348 153 366 153 405 129 405 91 405 66 305 60 285 60 284 58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3897.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4286.4,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(5138.4,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341 56 388 88 425 132 442 175 435 205 417 221 395 229 376L231 369Q231 367 232 367L243 378Q303 442 384 442 401 442 415 440T441 433 460 423 475 411 485 398 493 385 497 373 500 364 502 357L510 367Q573 442 659 442 713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145 857 144 853 130 845 101 831 73T785 17 716-10Q669-10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291 466 157Q433 26 428 16 415-11 385-11 372-11 364-4T353 8 350 18Q350 29 384 161L420 307Q423 322 423 345 423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 181 151 335 151 342 154 357 154 369 154 405 129 405 107 405 92 377T69 316 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392 386 422 416 422 429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35 443 55 463 131 469 151 473 152 475 153 483 153H487Q506 153 506 144 506 138 501 117T481 63 449 13Q436 0 417-8 409-10 393-10 359-10 336 5T306 36L300 51Q299 52 296 50 294 48 292 46 233-10 172-10 117-10 75 30T33 157ZM351 328Q351 334 346 350T323 385 277 405Q242 405 210 374T160 293Q131 214 119 129 119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442 467 442 494 420T522 361Q522 332 508 314T481 292 458 288Q439 288 427 299T415 328Q415 374 465 391 454 404 425 404 412 404 406 402 368 386 350 336 290 115 290 78 290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145 504 144 502 134 486 77 440 33T333-11Q263-11 227 52 186-10 133-10H127Q78-10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101 142 81 130 66T107 46 94 41L91 40Q91 39 97 36T113 29 132 26Q168 26 194 71 203 87 217 139T245 247 261 313Q266 340 266 352 266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1979,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2368,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(3220,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3831.2,0)"><path data-c="2212" d="M84 237T84 250 98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(4831.4,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341 56 388 88 425 132 442 175 435 205 417 221 395 229 376L231 369Q231 367 232 367L243 378Q303 442 384 442 401 442 415 440T441 433 460 423 475 411 485 398 493 385 497 373 500 364 502 357L510 367Q573 442 659 442 713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145 857 144 853 130 845 101 831 73T785 17 716-10Q669-10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291 466 157Q433 26 428 16 415-11 385-11 372-11 364-4T353 8 350 18Q350 29 384 161L420 307Q423 322 423 345 423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 181 151 335 151 342 154 357 154 369 154 405 129 405 107 405 92 377T69 316 57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5709.4,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369 98 420 158 442Q197 442 223 419T250 357Q250 340 236 301T196 196 154 83Q149 61 149 51 149 26 166 26 175 26 185 29T208 43 235 78 260 137Q263 149 265 151T282 153Q302 153 302 143 302 135 293 112T268 61 223 11 161-11Q129-11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281 56 279 53 278 49 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6054.4,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341 56 388 89 425 135 442Q171 442 195 424T225 390 231 369Q231 367 232 367L243 378Q304 442 382 442 436 442 469 415T503 336 465 179 427 52Q427 26 444 26 450 26 453 27 482 32 505 65T540 145Q542 153 560 153 580 153 580 145 580 144 576 130 568 101 554 73T508 17 439-10Q392-10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291 189 157Q156 26 151 16 138-11 108-11 95-11 87-5T76 7 74 17Q74 30 112 180T152 343Q153 348 153 366 153 405 129 405 91 405 66 305 60 285 60 284 58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(6654.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488 164 576 202 643 244 695 277 729 302 750H315 319Q333 750 333 741 333 738 316 720T275 667 226 581 184 443 167 250 184 58 225-81 274-167 316-220 333-241Q333-250 318-250H315 302L274-226Q180-141 137-14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7043.4,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11 26 15 29 27 33 41 36 43T55 46Q141 49 190 98 200 108 306 224T411 342Q302 620 297 625 288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681 380 681 408 681T453 682 473 682Q490 682 490 671 490 670 488 658 484 643 481 640T465 637Q434 634 411 620L488 426 541 485Q646 598 646 610 646 628 622 635 617 635 609 637 594 637 594 648 594 650 596 664 600 677 606 683H618Q619 683 643 683T697 681 738 680Q828 680 837 683H845Q852 676 852 672 850 647 840 637H824Q790 636 763 628T722 611 698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142 638 56Q648 47 699 46 734 46 734 37 734 35 732 23 728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10 444 11 446 25 448 35 450 39T455 44 464 46 480 47 506 54Q523 62 523 64 522 64 476 181L429 299Q241 95 236 84 232 76 232 72 232 53 261 47 262 47 267 47T273 46Q276 46 277 46T280 45 283 42 284 35Q284 26 282 19 279 6 276 4T261 1Q258 1 243 1T201 2 142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(7895.4,0)"><path data-c="29" d="M60 749 64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12 224-76 186-143 145-194 113-227 90-246Q87-249 86-250H74Q66-250 63-250T58-247 55-238Q56-237 66-225 221-64 221 250T66 725Q56 737 55 738 55 746 60 749Z"></path></g></g><rect width="8484.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.MinMaxScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br></pre></td></tr></table></figure><p>数据经过 Min-max Normalization 会被缩放到 <code>[0, 1]</code>。使用标准化还是 Min-max 归一化？这个问题没有标准答案，需要具体问题具体分析，<strong>通常情况下使用标准化较多</strong>。数据存在较多异常值也考虑使用标准化；Min-max 常用于归一化图像的灰度值。决策树则不需要特征缩放！吴恩达给的建议是反正使用标准化也没有坏处，就都用上。</p><h2 id="归一化">归一化</h2><p>在 <code>sklearn</code> 中特征缩放都被称为标准化(Standardizatoin)，Z-score 也被称为去均值和方差按比例缩放，其他的都是将特征缩放到给定的最小值和最大值之间(Rescaling，按比例缩放)。而归一化指的是缩放单个样本以具有单位范数的过程，在量化任何样本间的相似度时非常有用。</p><p>使用以下代码可以将单个样本的一范数或者二范数归一化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preprocessing.normalize(X, norm=<span class="string">'l1'</span>)</span><br><span class="line">preprocessing.normalize(X, norm=<span class="string">'l2'</span>)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li><li><a target="_blank" rel="noopener" href="https://www.quora.com/Why-does-mean-normalization-help-in-gradient-descent3">Why does mean normalization help in gradient descent?</a></li><li><a target="_blank" rel="noopener" href="https://www.robertoreif.com/blog/2017/12/16/importance-of-feature-scaling-in-data-modeling-part-1-h8nla">Importance of Feature Scaling in Data Modeling (Part 1)</a></li><li><a target="_blank" rel="noopener" href="https://www.robertoreif.com/blog/2017/12/21/importance-of-feature-scaling-in-data-modeling-part-2">Importance of Feature Scaling in Data Modeling (Part 2)</a></li><li><a target="_blank" rel="noopener" href="http://sklearn.apachecn.org/cn/0.19.0/modules/preprocessing.html">Sklearn Preprocessing data</a></li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"><img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://pengzhendong.github.io/2018/05/31/feature-scaling/" title="特征缩放">https://pengzhendong.github.io/2018/05/31/feature-scaling/</a></li><li class="post-copyright-license"><strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"><a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/05/31/gradient-checking/" rel="prev" title="梯度检验"><i class="fa fa-chevron-left"></i> 梯度检验</a></div><div class="post-nav-item"><a href="/2018/06/06/optimization-algorithms/" rel="next" title="优化算法">优化算法<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener("tabs:register",()=>{let{activeClass:t}=CONFIG.comments;if(CONFIG.comments.storage&&(t=localStorage.getItem("comments_active")||t),t){let e=document.querySelector(`a[href="#comment-${t}"]`);e&&e.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{if(!t.target.matches(".tabs-comment .tab-content .tab-pane"))return;let e=t.target.classList[1];localStorage.setItem("comments_active",e)})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="nav-number">2.</span> <span class="nav-text">特征缩放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">4.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">35</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https:&#x2F;&#x2F;www.facebook.com&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https:&#x2F;&#x2F;t.me&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https:&#x2F;&#x2F;weibo.com&#x2F;qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" title="关于 → &#x2F;about"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"><img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">230k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:29</span></div><script>!function(){function e(e){return e=encodeURI(e),document.getElementById(e).querySelector(".leancloud-visitors-count")}let{app_id:t,app_key:o,server_url:n}={enable:!0,app_id:"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz",app_key:"JRfKfM8mRPgxMB9GOSAnix9W",server_url:null,security:!0};function r(n){var r=(e,r,l)=>fetch(`${n}/1.1${r}`,{method:e,headers:{"X-LC-Id":t,"X-LC-Key":o,"Content-Type":"application/json"},body:JSON.stringify(l)});if(CONFIG.page.isPost){if(CONFIG.hostname!==location.hostname)return;!function(t){var o=document.querySelector(".leancloud_visitors"),n=decodeURI(o.id);o.dataset.flagTitle,t("get","/classes/Counter?where="+encodeURIComponent(JSON.stringify({url:n}))).then(e=>e.json()).then(({results:o})=>{if(o.length>0){var r=o[0];e(n).innerText=r.time+1,t("put","/classes/Counter/"+r.objectId,{time:{__op:"Increment",amount:1}}).catch(e=>{console.error("Failed to save visitor count",e)})}else e(n).innerText="Counter not initialized! More info at console err msg.",console.error("ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.")}).catch(e=>{console.error("LeanCloud Counter Error",e)})}(r)}else document.querySelectorAll(".post-title-link").length>=1&&function(t){var o=[...document.querySelectorAll(".leancloud_visitors")].map(e=>decodeURI(e.id));t("get","/classes/Counter?where="+encodeURIComponent(JSON.stringify({url:{$in:o}}))).then(e=>e.json()).then(({results:t})=>{for(let n of o){let o=t.find(e=>e.url===n);e(n).innerText=o?o.time:0}}).catch(e=>{console.error("LeanCloud Counter Error",e)})}(r)}let l="-MdYXbMMI"!==t.slice(-9)?n:`https://${t.slice(0,8).toLowerCase()}.api.lncldglobal.com`;l?r(l):fetch("https://app-router.leancloud.cn/2/route?appId="+t).then(e=>e.json()).then(({api_server:e})=>{r("https://"+e)})}()</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var e,t,o=document.getElementsByTagName("link");if(o.length>0)for(i=0;i<o.length;i++)"canonical"==o[i].rel.toLowerCase()&&o[i].href&&(e=o[i].href);t=e?e.split(":")[0]:window.location.protocol.split(":")[0],e||(e=window.location.href),function(){var i=e,o=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(i)){var n="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),i&&(n+="&l="+i)):i&&(n+="?l="+i),(new Image).src=n}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams"},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const a=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],a),d=document.createTextNode("");t.parentNode.replaceChild(d,t),n.start={node:d,delim:"",n:0},n.end={node:d,delim:"",n:0},e.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script></body></html>