---
title: æ·±åº¦ç¥ç»ç½‘ç»œ
date: 2018-05-21 13:56:22
updated: 2018-05-21 15:54:16
tags: Deep Learning
mathjax: true
typora-root-url: ./deep-neuron-network
---

## å‰è¨€

ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªéšè—å±‚ï¼Ÿéšè—å±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡è¶Šå¤šæ‹Ÿåˆèƒ½åŠ›ä¸å°±è¶Šå¼ºå—ï¼Ÿè¿™ä¸ªé—®é¢˜å›°æƒ‘äº†æˆ‘å¥½ä¹…ï¼Œè¯´ç™½äº†å°±æ˜¯ä¹¦è¯»çš„å¤ªå°‘ï¼Œæƒ³å¾—å¤ªå¤šã€‚å´æ©è¾¾ç”¨ç”µè·¯ç†è®ºå’ŒäºŒå‰æ ‘è§£å†³äº†æˆ‘è¿™ä¸ªå›°æƒ‘ï¼

<!-- more -->

## ç”µè·¯ç†è®ºå’Œæ·±åº¦å­¦ä¹ 

> There are functions you can compute with a "small" L-layer deep nerual network that shallower networks require exponentiall more hidden units to compute.

ä¹Ÿå°±æ˜¯è¯´æœ‰ä¸€äº›å‡½æ•°ï¼Œä¸€ä¸ªå¾ˆå°çš„ L å±‚æ·±åº¦ç¥ç»ç½‘ç»œå°±èƒ½å®ç°ï¼Œè€Œæµ…å±‚ç¥ç»ç½‘ç»œéœ€è¦çš„ç¥ç»å…ƒçš„æ•°é‡æ˜¯æŒ‡æ•°çº§åˆ«çš„ã€‚ä¾‹å¦‚å¼‚æˆ–æ“ä½œï¼Œå¯¹äºä¸‰ç»´æ•°æ®ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆä¸ºï¼š$x_1\oplus x_2\oplus x_3=(x_1\oplus x_2)\oplus x_3$ï¼Œæµ…å±‚ç¥ç»ç½‘ç»œæ‹Ÿåˆä¸ºï¼š$x_1\oplus x_2\oplus x_3=x_1\cdot x_2\cdot x_3+x'_1\cdot x'_2\cdot x_3+x'_1\cdot x_2\cdot x'_3+x_1\cdot x'_2\cdot x'_3$ï¼›æ‰€ä»¥æ·±åº¦ç¥ç»ç½‘ç»œçš„å±‚æ•°ä¹Ÿå°±æ˜¯äºŒå‰æ ‘çš„é«˜åº¦ $O(logn)$ï¼Œç¥ç»å…ƒçš„æ•°é‡ä¸ä¼šå¾ˆå¤§ï¼Œè€Œå•éšå±‚ç¥ç»ç½‘ç»œéœ€è¦çš„ç¥ç»å…ƒçš„ä¸ªæ•°åˆ™æ˜¯ $2^{n-1}$ ä¸ªï¼ŒæŒ‡æ•°çˆ†ç‚¸ï¼

## æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹

æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹å’Œå•éšå±‚ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ¨¡å—ä¸€æ ·ï¼Œåªä¸è¿‡æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„éšè—å±‚ä¸æ­¢ä¸€ä¸ªã€‚åœ¨å•éšå±‚ç¥ç»ç½‘ç»œçš„éšè—å±‚ä¸­ä½¿ç”¨äº† `Tanh` æ¿€æ´»å‡½æ•°ï¼Œè€Œç°åœ¨æ›´åŠ å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯ `ReLU` (çº¿æ€§æ•´æµ)å‡½æ•°ã€‚

### ReLU

ReLU å‡½æ•°æ˜¯ä¸€ä¸ªåˆ†æ®µå‡½æ•°ï¼Œå…¶å‡½æ•°å›¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](/ReLU.png)
$$
ReLU(x)=max(0, x)
$$
è¿™æ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œå½“ $x<0$ æ—¶ï¼Œ$ReLu(x)=0$ï¼Œæ¢¯åº¦ä¸º 0ï¼›å½“ $x\geq 0$ æ—¶ï¼Œ$ReLu(x)=x$ï¼Œæ¢¯åº¦ä¸º 1ã€‚

#### Squashing å‡½æ•°

ç¬¬ä¸€æ¬¡çœ‹åˆ° ReLU å‡½æ•°ï¼Œå°±è§‰å¾—å®ƒè™½ç„¶æ˜¯éçº¿æ€§çš„ï¼Œä½†æ˜¯å®ƒä¸æ˜¯ Squashing å‡½æ•°å•Šï¼å¯ä»¥é€šè¿‡ä¸¤ä¸ª ReLU ç¥ç»å…ƒçš„å åŠ ï¼Œæ„é€ ä¸€ä¸ª Squashing å‡½æ•°ï¼š
$$
\Psi(x)=ReLU(x)-ReLU(x-1)=max(0, x)-max(0, x-1)
$$
ä½¿ç”¨ ReLU å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°çš„æœ€å¤§å¥½å¤„æ˜¯æ¿€æ´»çŠ¶æ€çš„ç¥ç»å…ƒçš„æ¢¯åº¦ä¸ä¼šæ¶ˆå¤±ï¼Œä¸”æ¢¯åº¦å›ºå®šå¯ä»¥åŠ å¿«å­¦ä¹ é€Ÿåº¦ï¼›å…¶æ¬¡ï¼Œå¯¹äº**æ¯ä¸ªæ ·æœ¬æ•°æ®**ï¼Œä¸€éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºä¸º 0 é€ æˆäº†ç½‘ç»œçš„ç¨€ç–æ€§ï¼Œç¼“è§£äº†è¿‡æ‹Ÿåˆé—®é¢˜çš„å‘ç”Ÿã€‚è™½ç„¶**æ¯ä¸ªæ ·æœ¬æ•°æ®**ç»è¿‡ç¥ç»ç½‘ç»œåçš„è¾“å‡ºéƒ½æ˜¯è¾“å…¥çš„çº¿æ€§ç»„åˆï¼Œä½†æ˜¯ä¸åŒçš„è¾“å…¥æ¿€æ´»çš„ç¥ç»å…ƒæ˜¯ä¸åŒçš„ï¼Œæ­£æ˜¯å› ä¸ºè¿™ç§å˜æ¢å¼•å…¥äº†éçº¿æ€§ã€‚ä¾‹å¦‚å•éšå±‚ç¥ç»ç½‘ç»œæ‹Ÿåˆ $f(x)=x^2$:

* ä¸¤ä¸ªç¥ç»å…ƒï¼š

$$
h_1(x)=ReLU(x)+ReLU(-x)=|x|
$$

* å››ä¸ªç¥ç»å…ƒï¼š
  $$
  h_2(x)=ReLU(x)+ReLU(-x)+2ReLU(x-1)+2ReLU(-x-1)
  $$

å¤šä¸ª ReLU ç¥ç»å…ƒå åŠ ç¡®å®å¯ä»¥æ‹Ÿåˆå‡ºå„ç§å½¢çŠ¶ï¼Œæ‰€ä»¥åªè¦ç¥ç»å…ƒä¸ªæ•°è¶³å¤Ÿå¤šï¼Œæ‹Ÿåˆå®é™…é—®é¢˜ä¸­çš„å‡½æ•°å°±å“å“æœ‰ä½™ã€‚

#### ç¥ç»å…ƒåæ­»

ReLU å‡½æ•°ä¹Ÿæœ‰å…¶ç¼ºç‚¹ï¼Œé‚£å°±æ˜¯ç¥ç»å…ƒå®¹æ˜“â€œåæ­»â€ã€‚å¦‚æœ**æ‰€æœ‰æ ·æœ¬æ•°æ®**éƒ½ä¸èƒ½æ¿€æ´»æŸä¸ªç¥ç»å…ƒ(å³ä¸ç®¡è¾“å…¥æ˜¯ä»€ä¹ˆï¼Œè¾“å‡ºéƒ½ä¸€æ ·)ï¼Œé‚£ä¹ˆ <font color="red">ReLU å‡½æ•°çš„æ¢¯åº¦ $g'()$ ä¸º 0</font>ï¼Œåœ¨åå‘ä¼ æ’­çš„æ—¶å€™å‚æ•°å°±ä¸ä¼šè¢«æ›´æ–°ï¼Œè¿­ä»£åè¿˜æ˜¯ä¸€æ ·ï¼š
$$
dW^{[l]} \propto g'(Z^{[l]})
$$
å­¦ä¹ ç‡è¿‡å¤§æˆ–è€…å‚æ•° $w_1$ çš„æ¢¯åº¦è¿‡å¤§ï¼Œ$w_1$ çš„å˜åŒ–å°±ä¼šå¾ˆå¤§ï¼ŒåŸæ¥ $w_1x_1+w_2x_2+b$ å¯¹äºä¸åŒçš„æ ·æœ¬æ•°æ®ï¼Œè¾“å‡ºå¯èƒ½æœ‰æ­£æœ‰è´Ÿï¼Œç°åœ¨å¾ˆæœ‰å¯èƒ½å°±åªå‡ºç°è´Ÿæ•°ã€‚å¯¹äºç¬¬ä¸€å±‚éšè—å±‚çš„ç¥ç»å…ƒï¼Œä¸€æ—¦åæ­»å°±å†ä¹Ÿæ— æ³•è¢«æ¿€æ´»ï¼›å¯¹äºç¬¬äºŒå±‚åŠä»¥åçš„ç¥ç»å…ƒï¼Œç”±äºå®ƒçš„è¾“å…¥(ä¸Šä¸€å±‚çš„è¾“å‡º)ä¹Ÿæ˜¯åˆ«çš„ç¥ç»å…ƒçš„è¾“å…¥ï¼Œæ‰€ä»¥ä¸Šä¸€å±‚çš„è¾“å‡ºæ›´æ–°åæœ‰å¯èƒ½å†æ¬¡æ¿€æ´»è¿™ä¸ªåæ­»çš„ç¥ç»å…ƒã€‚

ReLU çš„å˜ç§ Leaky ReLU å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šå…‹æœç¥ç»å…ƒåæ­»çš„é—®é¢˜ã€‚ç”±äºåœ¨ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ—¶å€™ï¼Œè®­ç»ƒæ•°æ®çš„ç»´åº¦æ¯”è¾ƒå¤§ï¼Œå¯¹äºéƒ¨åˆ†ç¥ç»å…ƒåæ­»è¿˜æ˜¯å¯ä»¥æ¥å—çš„ã€‚

#### å‚æ•°åˆå§‹åŒ–

æ—¢ç„¶ ReLU å‡½æ•°é¿å…äº†è¿‡é¥±å’Œï¼Œé‚£ä¹ˆåœ¨åˆå§‹åŒ–å‚æ•°çš„æ—¶å€™ä¸ºä»€ä¹ˆè¿˜è¦ä» (0, 1) æ­£æ€åˆ†å¸ƒé‡ŒæŠ½æ ·å‘¢ï¼Ÿé¦–å…ˆåˆ†æä¸€ä¸‹ä»¥ä¸‹å‡ ç§æƒ…å†µï¼š

* $W$ éƒ½åˆå§‹åŒ–æˆç»å¯¹å€¼å¤§äº 1 çš„æ•°ï¼šæœ€åè¾“å…¥ Sigmoid å‡½æ•°çš„å€¼å°±ä¼šæŒ‡æ•°çˆ†ç‚¸ğŸ’¥ï¼Œ$sigmoid(x)=\frac{1}{1+e^{-x}}$ï¼Œè€Œ `np.exp(710)` æº¢å‡ºï¼›ä»£ä»·å‡½æ•°ä¸­åŒ…å« $log(1-a)$ï¼Œå…¶ä¸­ $a=sigmoid(x)$ï¼Œè€Œ `np.exp(-37)=1` å¯¼è‡´ä»£ä»·å‡½æ•°åŒ…å« $log(0)$ äº§ç”Ÿè¿è¡Œæ—¶è­¦å‘Šã€‚

* å› æ­¤ $W$ éœ€è¦åœ¨ $(-1, 1)$ ä¹‹é—´é‡‡æ ·ï¼Œå‰é¢åˆ†æè¿‡ä¸èƒ½éƒ½åˆå§‹åŒ–ä¸º 0ï¼›å¦‚æœå…¨éƒ¨åœ¨ $(-1, 0)$ æˆ–è€… $(0, 1)$ ä¹‹é—´é‡‡æ ·åˆ™ ReLU å‡½æ•°æ˜¯çº¿æ€§çš„ï¼Œå­¦ä¹ èƒ½åŠ›è¾ƒå·®ã€‚

* $W$ éƒ½åˆå§‹åŒ–æˆæ»¡è¶³ $(0, 1)$ æ­£æ€åˆ†å¸ƒçš„ç»å¯¹å€¼**ç‰¹åˆ«å°**çš„æ•°ï¼š$dW^{[l]} \propto W^{[l+1]}$ï¼Œåœ¨æ·±åº¦ç½‘ç»œä¸­æ¢¯åº¦ä¼šæŒ‡æ•°çº§é€’å‡å¼•å‘æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚

* $Z=\sum\limits_{i = 0}^{n}w_ix_i$ï¼Œç¥ç»å…ƒçš„ä¸ªæ•° $n$ è¶Šå¤§ï¼Œä¸‹ä¸€å±‚ç¥ç»ç½‘ç»œçš„è¾“å…¥å’Œè¾“å…¥çš„æ–¹å·®ä¹Ÿå°±è¶Šå¤§($w_i$ å’Œ $x_i$ åŒ 0 å‡å€¼åˆ†å¸ƒ)ï¼š
  $$
  \begin{align}
  Var(Z) &= Var(\sum\limits_{i = 0}^{n}w_ix_i) \\\
  &= \sum\limits_{i = 0}^{n}Var(w_ix_i) \\\
  &= \sum\limits_{i = 0}^{n}[E(w_i)]^2Var(x_i)+[E(x_i)]^2Var(w_i)+Var(x_i)Var(w_i) \\\
  &= \sum\limits_{i = 0}^{n}Var(x_i)Var(w_i) \\\
  &= nVar(w_i)Var(x_i)
  \end{align}
  $$
  æ‰€ä»¥ $n$ è¶Šå¤§æˆ‘ä»¬å¸Œæœ› $w_i$ è¶Šå°ï¼Œè¿™æ ·ä¸‹ä¸€å±‚ç¥ç»ç½‘ç»œçš„è¾“å…¥å’Œè¯¥è¾“å…¥çš„æ–¹å·®éƒ½ä¸ä¼šå¤ªå¤§ï¼Œè¾“å…¥å°±è¿˜æ˜¯ 0 é™„è¿‘æ¯”è¾ƒå°çš„æ•°ã€‚æ—¢ä¸ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ã€‚ 

å› æ­¤å‚æ•°æ—¢ä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå°ï¼Œæ‰€ä»¥å‚æ•°çš„åˆå§‹åŒ–å¾ˆé‡è¦ï¼ä¸€ç§æ–¹æ³•æ˜¯è®©æ¯å±‚ç¥ç»ç½‘ç»œçš„è¾“å…¥çš„æ–¹å·®å’Œè¾“å…¥å±‚çš„æ–¹æ³•ä¸€è‡´ï¼Œè¿™ç§æ–¹æ³•è™½ç„¶ä¸èƒ½å½»åº•è§£å†³é—®é¢˜ï¼Œä½†æ˜¯å¾ˆæœ‰æ•ˆã€‚å³ $Var(Z)=Var(x_i)$ï¼Œæ‰€ä»¥ $Var(w_i)=\frac{1}{n}$ã€‚å› ä¸º $Var(cw)=c^2Var(w)$ï¼Œæ‰€ä»¥åœ¨æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„åŸºç¡€ä¸Šä¹˜ä»¥ $\frac{1}{\sqrt{n}}$ å³å¯ä¿è¯ $w$ çš„æ–¹å·®ä¸º $\frac{1}{n}$ã€‚

``` python
params['W' + str(l)] = np.random.randn(laye_dims[l], laye_dims[l-1]) * np.sqrt(1/layer_dims[l-1])
```

##### Xavier åˆå§‹åŒ–

æ–¹æ³•åŒæ—¶è€ƒè™‘äº†åå‘ä¼ æ’­æ—¶çš„æƒ…å½¢ï¼Œæ­¤æ—¶çš„è¾“å…¥æ˜¯å‰å‘ä¼ æ’­çš„è¾“å‡ºï¼Œå› æ­¤ $Var(w_i)=\frac{1}{n}=\frac{1}{n_{out}}$ï¼Œäºæ˜¯ç»“åˆä»¥ä¸Šä¸¤ç‚¹è¦æ±‚ï¼Œæœ‰ $Var(w_i)=\frac{2}{n_{in}+n_{out}}$ã€‚

``` python
params['W' + str(l)] = np.random.randn(laye_dims[l], laye_dims[l-1]) * np.sqrt(2/(layer_dims[l-1]+layer_dims[l-1]))
```
åœ¨å´æ©è¾¾çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å»ºè®®å¦‚æœä½¿ç”¨ Tanh æ¿€æ´»å‡½æ•°ï¼Œåˆ™åˆå§‹åŒ–å‚æ•°æ–¹å·®ä¸º $\frac{1}{n}$ æˆ–è€… $\frac{2}{n_{in}+n_{out}}$ï¼›å¦‚æœä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ï¼Œä¼šå‘ç°æ•ˆæœå¹¶ä¸å¥½ï¼Œå› ä¸º ReLU æ¿€æ´»å‡½æ•°æœ‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºæ˜¯ 0(å³æ²¡æœ‰è¢«æ¿€æ´»)ï¼Œäºæ˜¯ä½•å‡¯æ˜ç­‰äººæå‡ºäº† MSRA åˆå§‹åŒ–çš„æ–¹æ³•ï¼Œä¹Ÿå« He åˆå§‹åŒ–ã€‚

##### He åˆå§‹åŒ–

He åˆå§‹åŒ–çš„æ€æƒ³æ˜¯ï¼šåœ¨ ReLU ç½‘ç»œä¸­ï¼Œå‡è®¾æœ‰ä¸€èˆ¬çš„ç¥ç»å…ƒè¢«æ¿€æ´»ï¼Œå¦ä¸€åŠè¾“å‡ºä¸º 0ï¼Œæ‰€ä»¥è¦ä¿æŒæ–¹å·®ä¸å˜åˆ™éœ€è¦åˆå§‹åŒ–å‚æ•°æ–¹å·®ä¸º $\frac{2}{n}$ã€‚è¿˜å¯ä»¥æŠŠåˆ†å­å½“æˆä¸€ä¸ªè¶…çº§å‚æ•°æ¥è°ƒèŠ‚ï¼Œä½†æ˜¯è¿™ä¸ªè¶…çº§å‚æ•°å¹¶ä¸æ˜¯å¾ˆé‡è¦ï¼Œæ‰€ä»¥ä¼˜å…ˆçº§å¯ä»¥æ”¾å¾—æ¯”è¾ƒä½ã€‚ç”±äºæ²¡æœ‰è€ƒè™‘åå‘ä¼ æ’­ï¼Œæ‰€ä»¥åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œè¿˜æ˜¯ä½¿ç”¨ Xavier åˆå§‹åŒ–æ–¹æ³•çš„æ¯”è¾ƒå¤šã€‚

### æ¨¡å‹ç»“æ„

æ„å»ºä¸€ä¸ª L å±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ éƒ¨åˆ†ï¼š

1. åˆå§‹åŒ– L å±‚ç¥ç»ç½‘ç»œçš„å‚æ•°
2. å®ç°å‰å‘ä¼ æ’­æ¨¡å‹(å›¾ä¸­ç´«è‰²éƒ¨åˆ†)
   * è®¡ç®—æ¯ä¸€å±‚å‰å‘ä¼ æ’­æ­¥éª¤çš„çº¿æ€§(LINEAR)éƒ¨åˆ†ï¼Œå³è®¡ç®— $Z^{[l]}$
   * ä½¿ç”¨æ¿€æ´»(ACTIVATION)å‡½æ•° `ReLU` æˆ–è€… `Sigmoid`
   * å°†ä¸¤ä¸ªæ­¥éª¤ç»“åˆåˆ°ä¸€ä¸ªæ–°çš„å‰å‘å‡½æ•°ä¸­ï¼š`[LINEAR->ACTIVATION]`
   * å‰ L-1 å±‚ï¼š `[LINEAR->ACTIVATION]`ï¼Œæœ€åä¸€å±‚ï¼š `[LINEAR->SIGMOID]`
3. è®¡ç®—æŸå¤±
4. å®ç°åå‘ä¼ æ’­æ¨¡å‹(å›¾ä¸­çº¢è‰²éƒ¨åˆ†)
   * è®¡ç®—æ¯ä¸€å±‚åå‘ä¼ æ’­æ­¥éª¤çš„çº¿æ€§(LINEAR)éƒ¨åˆ†
   * ä½¿ç”¨æ¿€æ´»(ACTIVATION)å‡½æ•° `ReLU` æˆ–è€… `Sigmoid` çš„æ¢¯åº¦
   * å°†ä¸¤ä¸ªæ­¥éª¤ç»“åˆåˆ°ä¸€ä¸ªæ–°çš„åå‘å‡½æ•°ä¸­ï¼š`[LINEAR->ACTIVATION]`
   * å‰ L-1 å±‚ï¼š `[LINEAR->ACTIVATION]`ï¼Œæœ€åä¸€å±‚ï¼š `[LINEAR->SIGMOID]`
5. æ›´æ–°å‚æ•°

![](/final outline.png)

#### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

å®éªŒä¸­çš„è®­ç»ƒæ•°æ®æ˜¯ 209 å¼  `64*64*3` çš„å›¾ç‰‡ï¼Œå˜æˆå‘é‡åçš„ X çš„ç»´åº¦æ˜¯ (12288, 209)ï¼Œå› æ­¤æ¨¡å‹å‚æ•°çš„ç»´åº¦å¦‚ä¸‹æ ‡æ‰€ç¤ºï¼š

|          | $W$ çš„ç»´åº¦               | $b$ çš„ç»´åº¦       | æ¿€æ´»å‡½æ•°çš„è¾“å…¥ $Z^{l}$                        | æ¿€æ´»å‡½æ•°çš„ç»´åº¦     |
| -------- | ------------------------ | ---------------- | --------------------------------------------- | ------------------ |
| Layer 1  | $(n^{[1]}, 12288)$       | $(n^{[1]}, 1)$   | $Z^{[1]} = W^{[1]}  X + b^{[1]}$              | $(n^{[1]}, 209)$   |
| Layer 1  | $(n^{[2]}, n^{[1]})$     | $(n^{[2]}, 1)$   | $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$         | $(n^{[2]}, 209)$   |
| $\vdots$ | $\vdots$                 | $\vdots$         | $\vdots$                                      | $\vdots$           |
| Layer 1  | $(n^{[L-1]}, n^{[L-2]})$ | $(n^{[L-1]}, 1)$ | $Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ | $(n^{[L-1]}, 209)$ |
| Layer 1  | $(n^{[L]}, n^{[L-1]})$   | $(n^{[L]}, 1)$   | $Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]}$       | $(n^{[L]}, 209)$   |

```python
def initialize_parameters_deep(layer_dims):
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims)            # number of layers in the network

    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))
        
    return parameters
```

å‚æ•° `layer_dims` æ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«äº†å®šä¹‰çš„æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚çš„ç¥ç»å…ƒçš„ä¸ªæ•°ã€‚

#### å‰å‘ä¼ æ’­æ¨¡å—

åœ¨çº¿æ€§éƒ¨åˆ†å’Œæ¿€æ´»å‡½æ•°éƒ¨åˆ†ï¼Œå‰å‘ä¼ æ’­éƒ½ä¼šç¼“å­˜æ‰€æœ‰è¾“å…¥ï¼Œç”¨äºåå‘ä¼ æ’­æ—¶è®¡ç®—æ¢¯åº¦ã€‚

* çº¿æ€§å‰å‘
  $$
  Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}, å…¶ä¸­ A^{[0]}=X
  $$

  ``` python
  def linear_forward(A, W, b):
      Z = np.dot(W, A) + b
  
      cache = (A, W, b)
      
      return Z, cache
  ```

* çº¿æ€§-æ¿€æ´»å‰å‘

  * Sigmoid: $g(Z)=\sigma(WA+b)=\frac{1}{1+e^{-(WA+b)}}$
  * ReLU: $g(Z)=ReLU(Z)=max(0, Z)$

  $$
  A^{[l]}=g(W^{[l]}A^{[l-1]}+b^{[l]})
  $$

  ``` python
  def linear_activation_forward(A_prev, W, b, activation):
      if activation == "sigmoid":
          Z, linear_cache = linear_forward(A_prev, W, b)
          A, activation_cache = sigmoid(Z)
      elif activation == "relu":
          Z, linear_cache = linear_forward(A_prev, W, b)
          A, activation_cache = relu(Z)
      
      cache = (linear_cache, activation_cache)
  
      return A, cache
  ```

* L å±‚å‰å‘æ¨¡å‹

  å¾ªç¯ä½¿ç”¨æ¿€æ´»å‡½æ•°æ˜¯ ReLU çš„ `linear_activation_forward` L-1 æ¬¡ï¼Œå†ä½¿ç”¨æ¿€æ´»å‡½æ•°æ˜¯ Sigmoid çš„ `linear_activation_forward` 1 æ¬¡ï¼Œå°±å¯ä»¥æ„å»ºä¸€ä¸ª L å±‚ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚åœ¨å®éªŒè¿‡ç¨‹ä¸­ï¼Œéœ€è¦æŠŠæ¯å±‚çš„ç¼“å­˜éƒ½æ”¾åˆ°åŒä¸€ä¸ªç¼“å­˜åˆ—è¡¨ä¸­ï¼Œç„¶åè¿”å›è¾“å‡ºå’Œç¼“å­˜ï¼Œç”¨äºè®¡ç®—ä»£ä»·å‡½æ•°å’Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€‚

  $$
  \hat Y=A^{[L]}=\sigma(W^{[L]}A^{[L-1]}+b^{[L]})
  $$
  
``` python
  def L_model_forward(X, parameters):
      caches = []
      A = X
      L = len(parameters) // 2                  # number of layers in the neural network
      
      # [LINEAR -> RELU]*(L-1)
      for l in range(1, L):
          A_prev = A 
          A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = "relu")
          caches.append(cache)
      
      # LINEAR -> SIGMOID
      AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = "sigmoid")
      caches.append(cache)
      
      return AL, caches
  ```

#### ä»£ä»·å‡½æ•°

$$
J=-\frac{1}{m}\sum\limits_{i=1}^{m}\left(y^{(i)}\log(a^{[L]\(i\)}) + (1-y^{(i)})\log(1- a^{[L]\(i\)})\right)
$$

```python
def compute_cost(AL, Y):
    m = Y.shape[1]
    cost = -np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), 1 - Y)) / m
    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).

    return cost
```

#### åå‘ä¼ æ’­æ¨¡å‹

åå‘ä¼ æ’­æ˜¯ç”¨æ¥è®¡ç®—ä»£ä»·å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•æ›´æ–°å‚æ•°åç»§ç»­å‰å‘ä¼ æ’­ï¼Œä½¿å¾—ä»£ä»·æ›´å°ã€‚åœ¨è®¡ç®—æ¢¯åº¦çš„æ—¶å€™éœ€è¦ç”¨åˆ°å‰å‘ä¼ æ’­ç¼“å­˜çš„è¾“å…¥ï¼š

* çº¿æ€§åå‘

  å‡è®¾å·²ç»è®¡ç®—å‡ºå¯¼æ•° $dZ^{[l]}=\frac{\partial \mathcal{L} }{\partial Z^{[l]}}$ï¼Œç°åœ¨éœ€è¦æ ¹æ® $dZ^{[l]} $ æ±‚ $dW^{[l]}, db^{[l]}, dA^{[l-1]}$ã€‚

  $$
  dW^{[l]}=\frac{\partial \mathcal{L}}{\partial W^{[l]}} = \frac{1}{m}dZ^{[l]}A^{[l-1]\mathrm{T}}
  $$
  
$$
  db^{[l]}=\frac{\partial \mathcal{L} }{\partial b^{[l]}}=\frac{1}{m}\sum_{i = 1}^{m}dZ^{[l]\(i\)}
  $$
  
$$
  dA^{[l-1]}=\frac{\partial \mathcal{L} }{\partial A^{[l-1]}}=W^{[l]\mathrm{T}}dZ^{[l]}
  $$
  
``` python
  def linear_backward(dZ, cache):
      A_prev, W, b = cache
      m = A_prev.shape[1]
  
      dW = np.dot(dZ, A_prev.T) / m
      db = np.sum(dZ, axis=1, keepdims=True) / m
      dA_prev = np.dot(W.T, dZ)
      
      return dA_prev, dW, db
  ```
  
* çº¿æ€§-æ¿€æ´»åå‘
  $$
  dZ^{[l]}= dA^{[l]} * g'(Z^{[l]})
  $$
  ReLU å‡½æ•°çš„å¯¼æ•°å°±æ˜¯ä¸€ä¸ªç®€å•çš„åˆ†æ®µå‡½æ•°ï¼Œå®éªŒç›´æ¥åœ¨ `dnn_utils` æ¨¡å—ä¸­å®ç°ï¼Œåªè¦è°ƒç”¨ä»¥ä¸‹å‡½æ•°ï¼Œä¼ å…¥ $dA^{[l]}$ å’Œå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„ç¼“å­˜å°±å¯ä»¥ç›´æ¥è¿”å› $dZ^{[l]}$:

  * Sigmoid: `dZ = sigmoid_backward(dA, activation_cache)`
  * ReLU: `dZ = relu_backward(dA, activation_cache)`

  ``` python
  def linear_activation_backward(dA, cache, activation):
      linear_cache, activation_cache = cache
      
      if activation == "relu":
          dZ = relu_backward(dA, activation_cache)
          dA_prev, dW, db = linear_backward(dZ, linear_cache)
          
      elif activation == "sigmoid":
          dZ = sigmoid_backward(dA, activation_cache)
          dA_prev, dW, db = linear_backward(dZ, linear_cache)
      
      return dA_prev, dW, db
  ```

* L å±‚åå‘æ¨¡å‹

  åœ¨åå‘ä¼ æ’­çš„æ—¶å€™ï¼Œé¦–å…ˆéœ€è¦è®¡ç®—ä»£ä»·å‡½æ•°å¯¹æ¨¡å‹è¾“å‡º $A^{[L]}$(å³ $\hat Y$) çš„æ¢¯åº¦(è®¡ç®—å…¬å¼è§[å•éšå±‚ç¥ç»ç½‘ç»œ](/2018/05/19/Neuron-network/))ï¼Œç„¶åè°ƒç”¨ `linear_activation_backward` å‡½æ•°ï¼Œæœ€åè¿”å›è®¡ç®—å‡ºçš„æ¢¯åº¦åˆ—è¡¨ï¼š

  ``` python
def L_model_backward(AL, Y, caches):
      grads = {}
      L = len(caches) # the number of layers
      m = AL.shape[1]
      Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL
      
      # Initializing the backpropagation
      dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
      
      # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]
      current_cache = caches[L-1]
      grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, activation = "sigmoid")
      
      # Loop from l=L-2 to l=0
      for l in reversed(range(L-1)):
          # lth layer: (RELU -> LINEAR) gradients.
          # Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] 
          current_cache = caches[l]
          dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(l + 2)], current_cache, activation = "relu")
          grads["dA" + str(l + 1)] = dA_prev_temp
          grads["dW" + str(l + 1)] = dW_temp
          grads["db" + str(l + 1)] = db_temp
  
      return grads
  ```

#### æ›´æ–°å‚æ•°

$$
W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}
$$

$$
b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}
$$

``` python
def update_parameters(parameters, grads, learning_rate): 
    L = len(parameters) // 2 # number of layers in the neural network

    # Update rule for each parameter. Use a for loop.
    for l in range(L):
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)]
        
    return parameters
```

### L å±‚ç¥ç»ç½‘ç»œ

åœ¨å®ç° L å±‚ç¥ç»ç½‘ç»œçš„å„ä¸ªæ¨¡å—åï¼Œç°åœ¨å°†å®ƒä»¬ç»„è£…æˆä¸€ä¸ª L å±‚ç½‘ç»œæ¨¡å‹ï¼Œé€šè¿‡ `layers_dims` æŒ‡å®šç½‘ç»œç»“æ„ï¼Œè®¾ç½®å­¦ä¹ ç‡ä¸º 0.0075 è¿­ä»£è®­ç»ƒæ•°æ® 3000 æ¬¡ï¼Œæœ€åè¿”å›è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ï¼š

``` python
def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009
    np.random.seed(1)
    costs = []                         # keep track of cost
    
    # Parameters initialization. (â‰ˆ 1 line of code)
    parameters = initialize_parameters_deep(layers_dims)
    
    # Loop (gradient descent)
    for i in range(0, num_iterations):

        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.
        AL, caches = L_model_forward(X, parameters)
        
        # Compute cost.
        cost = compute_cost(AL, Y)
    
        # Backward propagation.
        grads = L_model_backward(AL, Y, caches)
 
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)
                
        # Print the cost every 100 training example
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)
            
    # plot the cost
    plt.plot(np.squeeze(costs))
    plt.ylabel('cost')
    plt.xlabel('iterations (per tens)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
```



## å‚è€ƒæ–‡çŒ®

[1] å´æ©è¾¾. DeepLearning. 

[2] Christopher Olah. Neural Networks, Manifolds, and Topology. 2014

[3] X. Glorot, Y. Bengio, "Understanding the Difficulty of Training Deep Feedforward Neural Networks",Â *Proc. Conf. Artificial Intelligence and Statistics*, 2010.