---
title: Emojify æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
date: 2018-08-31 10:18:33
updated: 2018-08-31 12:18:57
tags: Deep Learning
mathjax: true
typora-root-url: ./emojify
---

## å‰è¨€

å†™è®ºæ–‡åšå®éªŒçš„æ—¶å€™æ›¾ç»æƒ³è¿‡ç”¨æ–‡æœ¬åˆ†ç±»çš„æ¨¡å‹ï¼Œæ— å¥ˆæ ·æœ¬å¤ªä¸å‡è¡¡ï¼Œæ‰€ä»¥æœ€åç”¨äº†è‡ªç¼–ç å™¨æå–ç‰¹å¾ã€‚åœ¨ Coursera çš„ä½œä¸šä¸­ï¼Œè¯¥å®éªŒåˆ†ä¸ºä¸¤ä¸ªå°å®éªŒï¼Œä¸€ä¸ªæ˜¯æ™®é€šçš„æ–‡æœ¬åˆ†ç±»ï¼Œä¸€ä¸ªæ˜¯ä½¿ç”¨ LSTM RNN è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚

<!-- more -->

## Baseline æ¨¡å‹: Emojifier-V1

è®­ç»ƒé›† X ä¸­åŒ…å« 127 ä¸ªå¥å­ï¼Œå…¶æ ‡ç­¾ä¸º 0 åˆ° 4 åˆ†åˆ«å¯¹åº”ä¸€ä¸ª emoji è¡¨æƒ…ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://s1.ax2x.com/2018/08/31/5BvpY3.png)

ç°åœ¨è½½å…¥æ•°æ®é›†ï¼Œå¹¶ä¸”æµ‹è¯•ä¸€ä¸‹ï¼š

``` python
X_train, Y_train = read_csv('data/train_emoji.csv')
X_test, Y_test = read_csv('data/tesss.csv')
maxLen = len(max(X_train, key=len).split())

index = 1
print(X_train[index], label_to_emoji(Y_train[index]))
```

```
I am proud of your achievements ğŸ˜„
```

### Emojifier-V1 æ¦‚å†µ

Emojifier-V1 çš„æ¦‚å†µå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://s1.ax2x.com/2018/08/31/5BydTN.png)

è¯¥æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œé¦–å…ˆå»è®­ç»ƒå¥½çš„ Embedding ä¸­æ‰¾åˆ°æ¯ä¸ªå•è¯çš„åµŒå…¥ï¼Œç„¶åå¯¹å¥å­ä¸­æ‰€æœ‰å•è¯çš„åµŒå…¥æ±‚å¹³å‡ï¼Œå°†å…¶ä½œä¸ºè¾“å…¥ï¼Œè¾“å…¥åˆ°ä¸€ä¸ªå¤šåˆ†ç±»çš„å…¨è¿æ¥ç½‘ç»œä¸­ï¼Œæœ€åé¢„æµ‹å¥å­çš„æƒ…æ„Ÿã€‚

### å®ç° Emojifier-V1

æ­¤å¤„ä¸å†ç»†è¿°å¤šåˆ†ç±»çš„è¿‡ç¨‹ï¼Œæ¨¡å‹çš„ä¸»è¦å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š
$$
z^{(i)} = W \times avg^{(i)} + b
$$

$$
a^{(i)} = softmax(z^{(i)})
$$

$$
\mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)
$$

å…¶ä¸­ $Yoh$ (Y one hot) æ˜¯è¾“å‡ºçš„ç‹¬çƒ­ç¼–ç ã€‚æœ€åæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡èƒ½å¤Ÿè¾¾åˆ° 97% å’Œ 86% ï¼ŒåŒæ—¶å¯¹äºä¸€äº›åœ¨è®­ç»ƒé›†ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„å•è¯ (ä¾‹å¦‚: adore) ä¹Ÿèƒ½å¾—åˆ°ä¸é”™çš„ç»“æœï¼š

``` python
X_my_sentences = np.array(["i adore you", "i love you", "funny lol", "lets play with a ball", "food is ready", "you are not happy"])
Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])

pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)
print_predictions(X_my_sentences, pred)
```

```
Accuracy: 0.8333333333333334

i adore you â¤ï¸
i love you â¤ï¸
funny lol ğŸ˜„
lets play with a ball âš¾
food is ready ğŸ´
you are not happy â¤ï¸
```

ä½†æ˜¯è¯¥æ¨¡å‹å¹¶ä¸èƒ½åˆ†æ not happy æ˜¯è¡¨ç¤ºä¸å¼€å¿ƒï¼Œè€Œåªæ˜¯ç®€å•åœ°å­¦ä¹ äº† happy è¿™ä¸ªå•è¯ã€‚è¾“å‡ºæ¨¡å‹çš„æ··æ·†çŸ©é˜µçœ‹ä¸€ä¸‹æ¨¡å‹çš„è¡¨ç°ï¼š

```python
print(Y_test.shape)
print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))
print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))
plot_confusion_matrix(Y_test, pred_test)
```

```
(56,)
           â¤ï¸   âš¾   ğŸ˜„   ğŸ˜  ğŸ´
Predicted  0.0  1.0  2.0  3.0  4.0  All
Actual                                 
0            6    0    0    1    0    7
1            0    8    0    0    0    8
2            2    0   16    0    0   18
3            1    1    2   12    0   16
4            0    0    1    0    6    7
All          9    9   19   13    6   56
```

![](https://s1.ax2x.com/2018/08/31/5Byc9X.png)

çŸ©é˜µå¯¹è§’çº¿ä¸Šçš„é¢œè‰²æ¯”è¾ƒæ·±ï¼Œè¡¨ç¤ºæ¨¡å‹çš„è¡¨ç°è¿˜ä¸é”™ã€‚ä½†æ˜¯æ¨¡å‹å´æ— æ³•åˆ†æ not xxx è¿™ç±»çš„çŸ­è¯­ï¼Œå› ä¸ºåµŒå…¥çŸ©é˜µä¸­æ²¡æœ‰å¯¹åº”çš„è¡¨ç¤ºï¼Œè€Œä¸”å•çº¯åœ°å¯¹æ‰€æœ‰å•è¯çš„åµŒå…¥æ±‚å¹³å‡ä¼šä¸¢å¤±è¾“å…¥çš„å•è¯çš„é¡ºåºï¼Œå› æ­¤éœ€è¦æ›´å¥½çš„ç®—æ³•ã€‚

## Emojifier-V2: åœ¨ Keras ä¸­ä½¿ç”¨ LSTMs

Emojifier-V2 çš„æ¦‚å†µå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://s1.ax2x.com/2018/08/31/5ByeQ6.png)

è¿™æ˜¯ä¸€ä¸ªä¸¤å±‚çš„ LSTM åºåˆ—åˆ†ç±»å™¨ã€‚è¿™æ¬¡å®éªŒä½¿ç”¨ mini-batches æ¥è®­ç»ƒ Kerasï¼Œå› æ­¤ä¸€ä¸ª batch ä¸­çš„åºåˆ—çš„é•¿åº¦åº”è¯¥ç›¸åŒï¼Œå› æ­¤éœ€è¦è¡¥ 0ã€‚ä¾‹å¦‚ä¸€ä¸ª batch ä¸­çš„åºåˆ—çš„æœ€å¤§é•¿åº¦ä¸º 5ï¼Œé‚£ä¹ˆ "I love you" è¿™ä¸ªå¥å­çš„è¡¨ç¤ºä¸º $(e_{i}, e_{love}, e_{you}, \vec{0}, \vec{0})$ã€‚

### Embedding å±‚

åœ¨ Keras ä¸­ï¼ŒåµŒå…¥çŸ©é˜µè¢«è¡¨ç¤ºæˆä¸€ä¸ªå±‚ï¼Œç„¶åå°†è¯çš„ç´¢å¼•åŒ¹é…æˆåµŒå…¥å‘é‡ã€‚åµŒå…¥çŸ©é˜µå¯ä»¥è¢«è®­ç»ƒå‡ºæ¥ï¼Œä¹Ÿå¯ä»¥ç”¨ä¸€ä¸ªè®­ç»ƒå¥½çš„çŸ©é˜µæ¥åˆå§‹åŒ–å®ƒã€‚`Embedding()` å±‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://s1.ax2x.com/2018/08/31/5ByAXK.png)

è¾“å‡ºæ˜¯ä¸€ä¸ª (batch size, max input length, dimension of word vectors) çš„çŸ©é˜µã€‚word_to_index çš„å®ç°å¦‚ä¸‹æ‰€ç¤ºï¼š

``` python
def sentences_to_indices(X, word_to_index, max_len):
    m = X.shape[0]                                   # number of training examples
    
    # Initialize X_indices as a numpy matrix of zeros and the correct shape
    X_indices = np.zeros((m, max_len))
    
    for i in range(m):                               # loop over training examples
        
        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.
        sentence_words = X[i].lower().split()
        
        # Initialize j to 0
        j = 0
        
        # Loop over the words of sentence_words
        for w in sentence_words:
            
            # Set the (i,j)th entry of X_indices to the index of the correct word.
            X_indices[i, j] = word_to_index[w]
            # Increment j to j + 1
            j += 1
            
    return X_indices
```

æ¥ä¸‹æ¥éœ€è¦å®ç°é¢„è®­ç»ƒçš„ Embedding å±‚ï¼Œå°†è®­ç»ƒå¥½çš„åµŒå…¥çŸ©é˜µè®¾ç½®åˆ° `Embedding()` å±‚çš„æƒå€¼ä¸­ï¼š

``` python
def pretrained_embedding_layer(word_to_vec_map, word_to_index):
    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)
    emb_dim = word_to_vec_map["cucumber"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)
    
    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)
    emb_matrix = np.zeros((vocab_len, emb_dim))
    
    # Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary
    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]

    # Define Keras embedding layer with the correct output/input sizes, make it trainable.
    # Use Embedding(...). Make sure to set trainable=False.
    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)

    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".
    embedding_layer.build((None,))
    
    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer
```

### æ„å»ºæ¨¡å‹

æ¥ä¸‹æ¥éœ€è¦æ„å»ºæ¨¡å‹ï¼Œæ¨¡å‹åˆ†ä¸ºï¼š

* è¾“å…¥å±‚: `Input((max_len, m), dtype='int32')`
* LSTM å±‚: `LSTM(hidden_units, return_sequence)(embeddings)`
* Dropout å±‚: `Dropout(keep_prob)(X)`
* å…¨è¿æ¥å±‚: `Dense(output_dimension)(X)`
* æ¿€æ´»å±‚: `Activation(activation_func)(X)`

``` python
def Emojify_V2(input_shape, word_to_vec_map, word_to_index):
    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).
    sentence_indices = Input(input_shape, dtype='int32')
    
    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    
    # Propagate sentence_indices through your embedding layer, you get back the embeddings
    embeddings = embedding_layer(sentence_indices)   
    
    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a batch of sequences.
    X = LSTM(128, return_sequences=True)(embeddings)
    # Add dropout with a probability of 0.5
    X = Dropout(0.5)(X)
    # Propagate X trough another LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a single hidden state, not a batch of sequences.
    X = LSTM(128, return_sequences=False)(X)
    # Add dropout with a probability of 0.5
    X = Dropout(0.5)(X)
    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.
    X = Dense(5)(X)
    # Add a softmax activation
    X = Activation('softmax')(X)
    
    # Create Model instance which converts sentence_indices into X.
    model = Model(inputs=sentence_indices, outputs=X)
    
    return model
```

æ„å»ºå¥½æ¨¡å‹åå¯ä»¥é€šè¿‡æ¨¡å‹çš„ `summary()` æ–¹æ³•æ¥æ£€æŸ¥æ¨¡å‹çš„æ¦‚è¦ (max_len = 10)ï¼š

``` python
model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)
model.summary()
```

``` 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 10)                0         
_________________________________________________________________
embedding_2 (Embedding)      (None, 10, 50)            20000050  
_________________________________________________________________
lstm_1 (LSTM)                (None, 10, 128)           91648     
_________________________________________________________________
dropout_1 (Dropout)          (None, 10, 128)           0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 645       
_________________________________________________________________
activation_1 (Activation)    (None, 5)                 0         
=================================================================
Total params: 20,223,927
Trainable params: 223,877
Non-trainable params: 20,000,050
_________________________________________________________________
```

ç”±äºåµŒå…¥çŸ©é˜µæ˜¯è®­ç»ƒå¥½çš„ `trainable = False`ï¼Œå› æ­¤æœ‰ 400,001 * 50 = 20,000,050 ä¸ªå‚æ•°æ˜¯ Non-trainable å‚æ•°ã€‚æ¥ä¸‹æ¥éœ€è¦ç¼–è¯‘æ¨¡å‹ï¼Œå®šä¹‰æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œæœ€åæ‹Ÿåˆæ¨¡å‹ï¼š

``` python
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)
Y_train_oh = convert_to_one_hot(Y_train, C = 5)
model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)
```

è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡èƒ½æ¥è¿‘ 100% å’Œ 91%ã€‚å¯¹äº not happy ä¹Ÿèƒ½å‡†ç¡®é¢„æµ‹ï¼š

``` python
x_test = np.array(['you are not happy'])
X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)
print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))
```

```
you are not happy ğŸ˜
```

å› ä¸º LSTM ç½‘ç»œå…·æœ‰é•¿çŸ­æœŸè®°å¿†ï¼Œæ‰€ä»¥èƒ½å¤Ÿå¾ˆå¥½åœ°é¢„æµ‹æŸäº›å•è¯çš„ç»„åˆã€‚

## æ€»ç»“

åœ¨ NLP ä»»åŠ¡ä¸­ï¼Œå¦‚æœè®­ç»ƒé›†æ¯”è¾ƒå°ï¼Œæ¯”è¾ƒé€‚åˆç›´æ¥ç”¨è®­ç»ƒå¥½çš„åµŒå…¥çŸ©é˜µè€Œä¸æ˜¯è‡ªå·±è®­ç»ƒä¸€ä¸ªã€‚åœ¨ RNN ä¸­ï¼Œå¦‚æœæƒ³ç”¨ mini-batches æé«˜æ•ˆç‡(çŸ©é˜µçš„è¿ç®—æ¯”å¾ªç¯å¿«)ï¼Œé‚£ä¹ˆå°±éœ€è¦å¯¹æ ·æœ¬è¿›è¡Œè¡¥ 0ã€‚`LSTM()` çš„ `return_sequence` å‚æ•°å†³å®šè¿”å›æ‰€æœ‰çš„éšè—çŠ¶æ€è¿˜æ˜¯åªè¿”å›æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚

## å‚è€ƒæ–‡çŒ®

1. å´æ©è¾¾. DeepLearning. 