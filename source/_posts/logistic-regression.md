---
title: Logistic å›å½’å’Œ Softmax å›å½’
date: 2018-04-26 20:24:05
updated: 2018-04-26 21:32:11
tags: Machine Learning
mathjax: true
typora-root-url: ./logistic-regression
---

## å‰è¨€

æœ€è¿‘åœ¨çœ‹å´æ©è¾¾çš„ DeepLearningï¼Œå­¦ä¹ äº†ä¸å°‘å…³äºæ·±åº¦å­¦ä¹ çš„çŸ¥è¯†ï¼Œæ­£å¥½å‚è€ƒç€ä½œä¸šçš„å†…å®¹æ€»ç»“ä¸€ä¸‹ï¼ŒæŒ–è¿™ä¸ªå‘å¿…é¡»å¾—å¡«ï¼Œå“ˆå“ˆã€‚

<!-- more -->

å‡ ä¹æ‰€æœ‰æ·±åº¦å­¦ä¹ çš„ä¹¦éƒ½æ˜¯ä» **Logistic Regression** å¼€å§‹è®²ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªæœ€ç®€å•çš„â€œç¥ç»ç½‘ç»œâ€ã€‚ä»¥å‰çœ‹åˆ°ç½‘ä¸Šæœ‰ä¸å°‘äººæŠŠè¿™ä¸ªè¯ç»„ç¿»è¯‘ä¸ºâ€œé€»è¾‘å›å½’â€ï¼Œä½†æ˜¯çœ‹åˆ°è¥¿ç“œä¹¦ğŸ‰ä¸Šè¯´ logistic != logic ï¼Œä½œè€…æŠŠè¿™ä¸ªç¿»è¯‘ä¸ºå¯¹æ•°å‡ ç‡å›å½’ï¼Œç®€ç§°å¯¹ç‡å›å½’ã€‚å¯¹ç‡å›å½’å¯ä»¥è§£å†³çº¿æ€§äºŒåˆ†ç±»é—®é¢˜ï¼Œæ¨å¹¿æˆ Softmax å›å½’å¯ä»¥è§£å†³çº¿æ€§å¤šåˆ†ç±»é—®é¢˜ã€‚

## Logistic å›å½’

çº¿æ€§å›å½’é¢„æµ‹çš„ y æ˜¯è¿ç»­çš„å€¼ï¼Œå¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›é¢„æµ‹çš„ y åªèƒ½å– 0/1 ä¸¤ä¸ªå€¼ã€‚ä¾‹å¦‚å‡è®¾è‚¿ç˜¤æ˜¯å¦æ˜¯æ¶æ€§è‚¿ç˜¤åªå’Œè‚¿ç˜¤çš„å¤§å°æœ‰å…³ï¼Œç„¶åç»™å®šè‚¿ç˜¤çš„å¤§å°ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦æ˜¯æ¶æ€§è‚¿ç˜¤ï¼Œ1 è¡¨ç¤ºæ­£ä¾‹(æ˜¯)ï¼Œ0 è¡¨ç¤ºè´Ÿä¾‹(å¦)ã€‚

> åˆ†ç±»æ˜¯ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œåœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œå½“è¾“å‡ºå˜é‡ Y å–æœ‰é™ä¸ªç¦»æ•£å€¼æ—¶ï¼Œé¢„æµ‹é—®é¢˜ä¾¿æˆä¸ºåˆ†ç±»é—®é¢˜ã€‚è¿™æ—¶ï¼Œè¾“å…¥å˜é‡ X å¯ä»¥æ˜¯ç¦»æ•£çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯è¿ç»­çš„ã€‚ç›‘ç£å­¦ä¹ ä»æ•°æ®ä¸­å­¦ä¹ ä¸€ä¸ªåˆ†ç±»æ¨¡å‹æˆ–åˆ†ç±»å†³ç­–å‡½æ•°ï¼Œç§°ä¸ºåˆ†ç±»å™¨(classifier)ã€‚åˆ†ç±»å™¨å¯¹æ–°çš„è¾“å…¥è¿›è¡Œè¾“å‡ºçš„é¢„æµ‹(prediction)ï¼Œç§°ä¸ºåˆ†ç±»(classification)ã€‚

å¦‚æœä½¿ç”¨çº¿æ€§å›å½’è§£å†³äºŒåˆ†ç±»ä»»åŠ¡ï¼Œå› ä¸ºè¾“å‡ºä¼šæœ‰å¤§äº 1 å’Œå°äº 0 çš„æ•°ï¼Œè§£å†³è¿™ä¸ªé—®é¢˜åªéœ€è¦å°†çº¿æ€§å›å½’çš„é¢„æµ‹å€¼ $h_\boldsymbol{\theta}(\boldsymbol{x})=\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x}$ æ˜ å°„åˆ°å€¼åŸŸä¸º (0, 1) çš„ç©ºé—´ä¸Šï¼Œå³ï¼š

$$h_\boldsymbol{\theta}(\boldsymbol{x})=g(\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x})=\frac{1}{1+e^{-\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x}}}$$

å› æ­¤ $ln\frac{h_\boldsymbol{\theta}(\boldsymbol{x})}{1-h_\boldsymbol{\theta}(\boldsymbol{x})}=\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x}$ï¼Œå®é™…ä¸Šæ˜¯åœ¨ç”¨çº¿æ€§å›å½’æ¨¡å‹çš„é¢„æµ‹ç»“æœå»é€¼è¿‘æ ·æœ¬çœŸå®æ ‡è®°ä¸ºæ­£ä¾‹å’Œåä¾‹çš„å¯èƒ½æ€§çš„æ¯”å€¼ï¼Œå³çœŸå®æ ‡è®°çš„å¯¹æ•°å‡ ç‡ã€‚$g(z)$ ä¹Ÿå«åš `Logistic function` æˆ–è€… `Sigmoid function`ã€‚

![](sigmoid.png)

è¯¥å‡½æ•°çš„å¯¼æ•°ä¸ºï¼š

$$g'(z)=\frac{d}{dz}\frac{1}{1+e^{-z}}=\frac{e^{-z}}{(1+e^{-z})^2}=g(z)\left(1-g(z)\right)$$

$z$ è¶‹äºæ­£æ— ç©·æ—¶ï¼Œ$g(z)$ è¶‹äº 1ï¼›$z$ è¶‹äºè´Ÿæ— ç©·æ—¶ï¼Œ$g(z)$ è¶‹äº 0ã€‚

$h_\boldsymbol{\theta}(\boldsymbol{x}) \geq 0.5$ å³ $\boldsymbol{\theta}^\mathrm{T}\boldsymbol{x}\geq0$ è¡¨ç¤ºæ˜¯æ¶æ€§è‚¿ç˜¤çš„å¯èƒ½æ€§å¤§äºç­‰äº 50%ï¼Œå› æ­¤å¯ä»¥ç»™å®šä¸€ä¸ªé˜ˆå€¼ä¾‹å¦‚å°±æ˜¯ 0.5ï¼Œæ¶æ€§è‚¿ç˜¤çš„å¯èƒ½æ€§å¤§äºç­‰äº 50% å°±åˆ¤å®šæ˜¯æ¶æ€§è‚¿ç˜¤ï¼Œå¦åˆ™åˆ¤å®šä¸æ˜¯æ¶æ€§è‚¿ç˜¤ã€‚å³
$$
y =
\begin{cases}
0 & h_\boldsymbol{\theta}(\boldsymbol{x}) < 0.5 \\\
1 & h_\boldsymbol{\theta}(\boldsymbol{x}) \geq 0.5
\end{cases}
$$
åœ¨åˆ¤å®šæ˜¯å¦ä¸ºæ¶æ€§è‚¿ç˜¤çš„æ—¶å€™ï¼ŒåŒ»ç”Ÿä¼šæ›´åŠ æ³¨é‡å¬å›ç‡è€Œä¸æ˜¯å‡†ç¡®ç‡ï¼Œâ€œå®å¯æ€é”™ä¹Ÿä¸æ”¾è¿‡â€ï¼Œæ‰€ä»¥é˜ˆå€¼å¯èƒ½ä¼šæ›´å°ä¸€äº›ï¼Œä¾‹å¦‚åªè¦æœ‰ 40% å¯èƒ½æ€§å°±è¦åˆ¤å®šä¸ºæ¶æ€§è‚¿ç˜¤ï¼Œç„¶åè¿›è¡Œæ²»ç–—ã€‚

åœ¨å¯¹ç‡å›å½’ä¸­å¦‚æœåƒçº¿æ€§å›å½’æ¨¡å‹ä¸€æ ·å°†å¹³æ–¹æŸå¤±ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œé‚£ä¹ˆç›®æ ‡å‡½æ•°ä¸ºï¼š

$$E_{\theta}=\sum_{i=1}^m(y^{(i)}-\frac{1}{1+e^{-\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x}^{(i)}}})^2$$

è¿™æ˜¯ä¸€ä¸ªéå‡¸å‡½æ•°ï¼Œä¸å®¹æ˜“æ±‚è§£ï¼Œå®¹æ˜“å¾—åˆ°å±€éƒ¨æœ€ä¼˜å€¼ã€‚åœ¨å¯¹ç‡å›å½’ä¸­ï¼Œç»å¸¸ä½¿ç”¨æœ€å¤§ä¼¼ç„¶çš„æ–¹æ³•ä¼°è®¡æ¨¡å‹çš„å‚æ•°ï¼Œå› æ­¤æŸå¤±å‡½æ•°æ˜¯åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¨å¯¼å¾—åˆ°çš„å¯¹æ•°ä¼¼ç„¶æŸå¤±å‡½æ•°ã€‚

## ä¸¤å¤§å­¦æ´¾çš„äº‰è®º

åœ¨å­¦ä¹ æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¹‹å‰å…ˆäº†è§£ä¸€ä¸‹é¢‘ç‡å­¦æ´¾å’Œè´å¶æ–¯å­¦æ´¾å¯¹ä¸–ç•Œçš„è®¤çŸ¥ã€‚å¯¹äº‹ç‰©å»ºæ¨¡æ—¶ï¼Œç”¨ $\theta$ è¡¨ç¤ºæ¨¡å‹çš„å‚æ•°ï¼Œè§£å†³é—®é¢˜çš„æœ¬è´¨å°±æ˜¯æ±‚ $\theta$ã€‚ä¾‹å¦‚é€šè¿‡æŠ›ç¡¬å¸ä¼°è®¡ç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡ $P(head)=\theta$ã€‚

æ¦‚ç‡å’Œä¼¼ç„¶éƒ½æ˜¯æŒ‡å¯èƒ½æ€§ï¼Œä½†æ˜¯åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œæ¦‚ç‡å’Œä¼¼ç„¶æœ‰æˆªç„¶ä¸åŒçš„ç”¨æ³•ã€‚

- æ¦‚ç‡ï¼šæè¿°äº†å·²çŸ¥å‚æ•° $\theta$ æ—¶çš„éšæœºå˜é‡çš„è¾“å‡ºç»“æœã€‚ä¾‹å¦‚å·²çŸ¥å‚æ•° $\theta=0.5$ ï¼Œæ±‚æŠ› m æ¬¡ç¡¬å¸å‡ºç° n æ¬¡æ­£é¢æœä¸Šçš„æ¦‚ç‡ã€‚
- ä¼¼ç„¶ï¼šæè¿°å·²çŸ¥éšæœºå˜é‡è¾“å‡ºç»“æœæ—¶ï¼ŒæœªçŸ¥å‚æ•° $\theta$ çš„å¯èƒ½å–å€¼ã€‚ä¾‹å¦‚å·²çŸ¥æŠ› 100 æ¬¡ç¡¬å¸å‡ºç° 50 æ¬¡æ­£é¢æœä¸Šï¼Œæ±‚å‚æ•° $\theta=x$ çš„ä¼¼ç„¶ç¨‹åº¦ã€‚

### é¢‘ç‡å­¦æ´¾

æ ¹æ®éšæœºé‡å¤äº‹ä»¶çš„**é¢‘ç‡**æ¥è€ƒå¯Ÿ**æ¦‚ç‡**ã€‚æŠ› 10 æ¬¡æœ‰ 4 æ¬¡æ­£é¢æœä¸Šï¼Œåˆ™ $\theta=0.4$ã€‚å½“æ•°æ®é‡è¶‹äºæ— ç©·æ—¶å°±å¯ä»¥å¾—åˆ°ç²¾å‡†çš„ä¼°è®¡ï¼Œå½“ç¼ºä¹æ•°æ®æ—¶åˆ™å¯èƒ½å‡ºç°ä¸¥é‡çš„åå·®(è¿‡æ‹Ÿåˆ)ï¼Œä¾‹å¦‚å¯¹äºä¸€æšå‡åŒ€çš„ç¡¬å¸ï¼ŒæŠ› 10 æ¬¡æœ‰ 10 æ¬¡æ­£é¢æœä¸Š(è¿™ç§æƒ…å†µçš„æ¦‚ç‡æ˜¯ $\frac{1}{2^{10}}$)ï¼Œé¢‘ç‡å­¦æ´¾ä¼šç›´æ¥ä¼°è®¡ $\theta=1$ï¼Œç„¶åé¢„æµ‹æŠ›è¿™ä¸ªç¡¬å¸ 100% æ­£é¢æœä¸Šã€‚

### è´å¶æ–¯å­¦æ´¾

æ ¹æ®å…ˆéªŒ(Prior)æ¦‚ç‡å’Œä¼¼ç„¶å‡½æ•°(Likelihood function)ï¼Œè®¡ç®—åéªŒ(Posterior)æ¦‚ç‡ã€‚

è´å¶æ–¯å…¬å¼å¦‚ä¸‹ï¼š

$$P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}$$

å‡è®¾æŠ› 10 æ¬¡ç¡¬å¸æ˜¯ä¸€æ¬¡å®éªŒï¼Œ$P(X)$ ç›¸å½“äºæ˜¯ä¸€ä¸ªå½’ä¸€åŒ–é¡¹ï¼Œæ‰€ä»¥ $P(\theta|X)\propto{P(X|\theta)P(\theta)}$ã€‚

#### å…ˆéªŒæ¦‚ç‡ $P(\theta)$

è§‚æµ‹åˆ°æ•°æ®ä¹‹å‰ï¼Œä¸€äº›å…³äºå‚æ•° $\theta$ çš„å‡è®¾ï¼Œå³å‚æ•° $\theta$ å–æŸä¸ªå€¼çš„æ¦‚ç‡ã€‚æ‰€ä»¥ $\theta$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œç¬¦åˆä¸€å®šçš„æ¦‚ç‡åˆ†å¸ƒã€‚å½“å…ˆéªŒåˆ†å¸ƒæ˜¯å‡åŒ€åˆ†å¸ƒæ—¶ï¼Œè´å¶æ–¯æ–¹æ³•ç­‰ä»·äºé¢‘ç‡æ–¹æ³•ã€‚ä¸€èˆ¬ä¼¯åŠªåˆ©åˆ†å¸ƒæŠŠå…ˆéªŒåˆ†å¸ƒé€‰æ‹©ä¸º Beta åˆ†å¸ƒï¼Œå› ä¸ºå®ƒæ­£æ¯”äº $\theta$ å’Œ $1-\theta$ çš„å¹‚æŒ‡æ•°ï¼Œé‚£ä¹ˆåéªŒåˆ†å¸ƒå°±ä¼šæœ‰å’Œå…ˆéªŒåˆ†å¸ƒç›¸åŒçš„å‡½æ•°å½¢å¼(å…±è½­æ€§)ï¼Œæ¥ä¸‹æ¥è§‚æµ‹åˆ°æ›´å¤šæ•°æ®æ—¶åéªŒåˆ†å¸ƒå°±å¯ä»¥æ‰®æ¼”å…ˆéªŒåˆ†å¸ƒçš„è§’è‰²(è¯¦æƒ…è§ PRML 2.1.1)ã€‚

ä¸‹å›¾ä¸º Beta åˆ†å¸ƒçš„å‡½æ•°åˆ†å¸ƒå›¾ï¼Œè¡¨ç¤ºå…³äºå‚æ•° $\theta$ çš„å‡è®¾ã€‚ä¾‹å¦‚æ™®é€šçš„ç¡¬å¸ï¼ŒBeta åˆ†å¸ƒçš„è¶…å‚æ•° a å’Œ b å¯ä»¥å– 10ï¼Œå³æŠ› 20 æ¬¡ç¡¬å¸åº”è¯¥ä¼šæœ‰ 10 æ¬¡æ­£é¢æœä¸Šå’Œ 10 æ¬¡åé¢æœä¸Šã€‚ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œå‚æ•° $\theta$ å– 0.5 æ—¶å…ˆéªŒæ¦‚ç‡æœ€å¤§ï¼Œå–å…¶ä»–å€¼æ—¶å…ˆéªŒæ¦‚ç‡æ¯”è¾ƒå°ã€‚

> å¯ä»¥ç®€å•åœ°æŠŠå…ˆéªŒæ¦‚ç‡ä¸­çš„è¶…å‚æ•° a å’Œ b åˆ†åˆ«çœ‹å‡º x = 1 å’Œ x = 0 çš„æœ‰æ•ˆè§‚æµ‹æ¬¡æ•°ã€‚

![](beta1.png)

å¦‚æœå¯¹å…³äºå‚æ•° $\theta$ çš„å‡è®¾çš„æŠŠæ¡æ›´å¤§ï¼Œå³æŠ› 100 æ¬¡ç¡¬å¸åº”è¯¥ä¼šæœ‰ 50 æ¬¡æ­£é¢æœä¸Šå’Œ 50 æ¬¡åé¢æœä¸Šã€‚é‚£ä¹ˆå‚æ•° $\theta$ å– 0.5 çš„æ¦‚ç‡å°±æ›´å¤§ï¼Œå–å…¶ä»–å€¼çš„æ¦‚ç‡å°±æ›´å°ã€‚

![](beta2.png)

#### ä¼¼ç„¶å‡½æ•° $P(X|\theta)$

å‡è®¾å‚æ•° $\theta$ å·²çŸ¥åè§‚æµ‹åˆ°å·²æœ‰æ•°æ®çš„æ¦‚ç‡ï¼Œæ˜¯å…³äºå‚æ•° $\theta$ çš„å‡½æ•°ã€‚ä¾‹å¦‚æŠ› 10 æ¬¡ç¡¬å¸æœ‰ 2 æ¬¡æ­£é¢æœä¸Šï¼Œé‚£ä¹ˆä¼¼ç„¶å‡½æ•°

$$P(X|\theta)=\binom{10}{2}\theta^2(1-\theta)^8$$

å‡½æ•°å›¾åƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œå‚æ•° $\theta$ å– 0.2 æ—¶ä¼¼ç„¶ç¨‹åº¦æœ€å¤§ï¼Œå–å…¶ä»–å€¼æ—¶ä¼¼ç„¶ç¨‹åº¦æ¯”è¾ƒå°ã€‚

![](beta3.png)

#### åéªŒæ¦‚ç‡ $P(\theta|X)$

é€šè¿‡ä¼¼ç„¶å‡½æ•°ä¿®æ­£åï¼Œå‚æ•° $\theta$ å–æŸä¸ªå€¼çš„æ¦‚ç‡ã€‚å‚æ•° $\theta$ çš„æ¦‚ç‡åˆ†å¸ƒå°±æ˜¯åéªŒåˆ†å¸ƒã€‚

å¯¹äºå…ˆéªŒåˆ†å¸ƒä¸ºè¶…å‚æ•° a = b = 10 çš„ Beta åˆ†å¸ƒï¼Œä¼¼ç„¶å‡½æ•° $P(X|\theta)=\binom{10}{2}\theta^2(1-\theta)^8$ ï¼Œå¯ä»¥ç®—å‡ºå¯¹åº”çš„åéªŒåˆ†å¸ƒæ˜¯è¶…å‚æ•°ä¸º a = 12ï¼Œb = 18 çš„ Beta åˆ†å¸ƒã€‚

> å®šé‡åœ°æè¿°ä¸ç¡®å®šæ€§ï¼Œå¹¶ä¸”æ ¹æ®å°‘é‡æ–°çš„æ•°æ®å¯¹ä¸ç¡®å®šæ€§è¿›è¡Œç²¾ç¡®çš„ä¿®æ”¹ï¼Œå¯¹æ¥ä¸‹æ¥è¦é‡‡å–çš„åŠ¨ä½œè¿›è¡Œä¿®æ”¹ï¼Œæˆ–è€…å¯¹æœ€ç»ˆçš„å†³ç­–è¿›è¡Œä¿®æ”¹ã€‚

åœ¨è´å¶æ–¯å­¦æ´¾å’Œé¢‘ç‡å­¦æ´¾çš„è§‚ç‚¹ä¸­ï¼Œä¼¼ç„¶å‡½æ•°éƒ½èµ·ç€é‡è¦çš„ä½œç”¨ï¼Œç„¶è€Œä½¿ç”¨çš„æ–¹å¼æœ‰ç€æœ¬è´¨çš„ä¸åŒã€‚é¢‘ç‡å­¦å®¶è§‚ç‚¹è®¤ä¸ºå‚æ•° $\theta$ æ˜¯ä¸€ä¸ªå›ºå®šçš„å‚æ•°ï¼Œé¢‘ç‡å­¦æ´¾å¹¿æ³›ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå‚æ•° $\theta$ çš„å€¼å°±æ˜¯ä½¿ä¼¼ç„¶å‡½æ•° $P(X|\theta)$ è¾¾åˆ°æœ€å¤§å€¼çš„ $\theta$ çš„å€¼ï¼›è´å¶æ–¯å­¦æ´¾åˆ™å¹¿æ³›ä½¿ç”¨æœ€å¤§åéªŒä¼°è®¡ï¼Œå‚æ•° $\theta$ çš„å€¼å°±æ˜¯ä½¿åéªŒåˆ†å¸ƒ $P(\theta|X)\propto{P(X|\theta)P(\theta)}$ è¾¾åˆ°æœ€å¤§å€¼çš„ $\theta$ çš„å€¼ã€‚

è´å¶æ–¯è§‚ç‚¹çš„ä¼˜ç‚¹æ˜¯åŒ…å«äº†å…ˆéªŒæ¦‚ç‡ï¼Œç›¸å½“äºåŠ äº†æ­£åˆ™åŒ–é¡¹ï¼Œé¿å…äº§ç”Ÿè¿‡æ‹Ÿåˆã€‚ä¸€ä¸ªå¸¦æœ‰åˆç†çš„å…ˆéªŒåˆ†å¸ƒçš„è´å¶æ–¯æ–¹æ³•ä¸ä¼šé¢„æµ‹æŠ›ä¸€æšæ™®é€šçš„ç¡¬å¸ä¼š 100% æ­£é¢æœä¸Šï¼Œä½†æ˜¯å¦‚æœå…ˆéªŒåˆ†å¸ƒé€‰æ‹©ä¸å¥½ï¼Œè´å¶æ–¯æ–¹æ³•ä¹Ÿä¼šæœ‰å¾ˆå¤§çš„å¯èƒ½ç»™å‡ºé”™è¯¯çš„ç»“æœã€‚

### æœ€å¤§ä¼¼ç„¶ä¼°è®¡(MLE)

Maximun Likelihood Estimation æ˜¯é¢‘ç‡å­¦æ´¾å¹¿æ³›ç”¨çš„ä¼°è®¡æ–¹æ³•ã€‚å‡è®¾æ•°æ® $X$æ˜¯ç‹¬ç«‹åŒåˆ†å‘å¸ƒçš„ä¸€ç»„æŠ½æ ·ã€‚é‚£ä¹ˆ MLE å¯¹ $\theta$ çš„ä¼°è®¡æ–¹æ³•å¯ä»¥å¦‚ä¸‹æ¨å¯¼ï¼š
$$
\begin{align}
\hat\\theta_{MLE} & = \arg \max P(X;\theta) \\\
 & = \arg \min -log P(X;\theta)
\end{align}
$$

è¿™é‡Œä¹‹æ‰€ä»¥ç”¨ $P(X;\theta)$ è€Œä¸æ˜¯ $P(X|\theta)$ æ˜¯å› ä¸ºé¢‘ç‡å­¦æ´¾è®¤ä¸ºå‚æ•° $\theta$ æ˜¯å›ºå®šçš„å€¼(åªæ˜¯å½“å‰æœªçŸ¥)è€Œä¸æ˜¯éšæœºå˜é‡ã€‚æœ€åè¦ä¼˜åŒ–çš„å‡½æ•°è¢«ç§°ä¸º Negative Log Likelihood (NLL)ã€‚

### æœ€å¤§åéªŒä¼°è®¡(MAP)

Maximum A Posteriori æ˜¯è´å¶æ–¯å­¦æ´¾å¹¿æ³›ä½¿ç”¨çš„ä¼°è®¡æ–¹æ³•ã€‚å‡è®¾æ•°æ® $X$ æ˜¯ç‹¬ç«‹åŒåˆ†å‘å¸ƒçš„ä¸€ç»„æŠ½æ ·ã€‚é‚£ä¹ˆ MAP å¯¹ $\theta$ çš„ä¼°è®¡æ–¹æ³•å¯ä»¥å¦‚ä¸‹æ¨å¯¼ï¼š
$$
\begin{align}
\hat\\theta_{MAP} & = \arg \max P(\theta|X) \\\
 & = \arg \min - log P(\theta|X) \\\
 & = \arg \min - log P(X|\theta)-log P(\theta)+log P(X) \\\
 & = \arg \min - log P(X|\theta)-log P(\theta)
\end{align}
$$


MLE å’Œ MAP åœ¨ä¼˜åŒ–æ—¶çš„ä¸åŒå°±æ˜¯åœ¨äºå…ˆéªŒé¡¹ $-log P(\theta)$ã€‚å‡è®¾åœ¨æŸæ¬¡å®éªŒä¸­ï¼Œå…ˆéªŒåˆ†å¸ƒæ˜¯æ ‡å‡†é«˜æ–¯åˆ†å¸ƒï¼Œå³å‚æ•° $\theta$ æ»¡è¶³æ ‡å‡†é«˜æ–¯åˆ†å¸ƒï¼Œåˆ™ $P(\theta) = Ce^{-\frac{\theta^2}{2}}$ï¼Œ$-log P(\theta) = C + \frac{\theta^2}{2}$ã€‚æ‰€ä»¥åœ¨ MAP ä¸­é€‰æ‹©æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä½œä¸ºå…ˆéªŒåˆ†å¸ƒæ—¶å°±ç­‰ä»·äºåœ¨ MLE ä¸­é‡‡ç”¨äº† L2 çš„æ­£åˆ™åŒ–é¡¹ã€‚

## ä»£ä»·å‡½æ•°

ç”±äºæ— æ³•ä½¿ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºä»£ä»·å‡½æ•°ï¼Œæ‰€ä»¥åˆ†æå½“çœŸå®æ ‡ç­¾ä¸º 1 æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ› $h_\boldsymbol{\theta}(\boldsymbol{x})$ å°½å¯èƒ½æ¥è¿‘äº $1^-$ ï¼Œå³ $-log(h_\boldsymbol{\theta}(\boldsymbol{x}))$ å°½å¯èƒ½æ¥è¿‘äº $0^+$ï¼Œä¹Ÿå°±æ˜¯æœ€å°åŒ–è´Ÿå¯¹æ•°ã€‚

![](cost.png)

åŒç†å¯æ„é€ æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š

$$
loss\left(h_\boldsymbol{\theta}(\boldsymbol{x}), y\right) =
\begin{cases}
-log\left(h_\boldsymbol{\theta}(\boldsymbol{x})\right) & y=1 \\\
-log\left(1-h_\boldsymbol{\theta}(\boldsymbol{x})\right) & y=0
\end{cases}
$$
åˆå¹¶å¾—æŸå¤±å‡½æ•°ä¸ºï¼š$loss\left(h_\boldsymbol{\theta}(\boldsymbol{x}), y\right)=-ylog\left(h_\boldsymbol{\theta}(\boldsymbol{x})\right)-(1-y)log\left(1-h_\boldsymbol{\theta}(\boldsymbol{x})\right)$

æ‰€ä»¥å¯¹æ•°ä¼¼ç„¶ä»£ä»·å‡½æ•°ä¸ºï¼š
$$
J(\boldsymbol{\theta}) =-\frac{1}{m}\sum_{i=1}^m\Big(y^{(i)}log\left(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})\right)+(1-y^{(i)})log\left(1-h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})\right)\Big)
$$
ç”± Sigmoid å‡½æ•°çš„æ€§è´¨å¯çŸ¥ $\frac{\partial{h_\boldsymbol{\theta}(\boldsymbol{x})}}{\partial{\theta_j}}=h_\boldsymbol{\theta}(\boldsymbol{x})(1-h_\boldsymbol{\theta}(\boldsymbol{x}))x_j$ ï¼Œæ‰€ä»¥åœ¨æ¢¯åº¦ä¸‹é™æ±‚æœ€ä¼˜å€¼æ—¶éœ€è¦ç”¨åˆ°çš„æ¢¯åº¦å¯ä»¥æ¨å¯¼ä¸ºï¼š
$$
\begin{align}
\nabla_{\theta_j}J(\boldsymbol{\theta}) & = \frac{\partial{J(\boldsymbol{\theta})}}{\partial{\theta_j}} \\\
& = -\frac{1}{m}\sum_{i=1}^m\left(\frac{y^{(i)}}{h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})}\frac{\partial{h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})}}{\partial{\theta_j}}-\frac{1-y^{(i)}}{1-h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})}\frac{\partial{h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})}}{\partial{\theta_j}}\right) \\\
& = \frac{1}{m}\sum_{i=1}^m\left(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})-y^{(i)}\right)x^{(i)}_j
\end{align}
$$
åŒ–ç®€åå‘ç°å’Œçº¿æ€§å›å½’æ¨¡å‹çš„ä»£ä»·å‡½æ•°ä¸€æ ·ã€‚

## Softmax å›å½’

Softmax å›å½’æ˜¯å¯¹ç‡å›å½’çš„æ¨å¹¿ï¼Œå½“ç±»åˆ«æ•° K=2 çš„æ—¶å€™ï¼ŒSoftmax å›å½’é€€åŒ–ä¸ºå¯¹ç‡å›å½’ã€‚ç”±äºåœ¨æ¨¡å‹ä¸­ä½¿ç”¨äº† Softmax å‡½æ•°ï¼Œæ¯”è¾ƒæ¸©å’Œ(soft)åœ°è¾“å‡ºæ ·æœ¬å±äºå„ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œè€Œä¸æ˜¯ç›´æ¥æœ€å¯èƒ½å±äºçš„ç±»åˆ«ï¼Œå› æ­¤å«åš Softmax å›å½’ã€‚

Softmax å‡½æ•°æˆ–ç§°**å½’ä¸€åŒ–æŒ‡æ•°å‡½æ•°**ï¼Œæ˜¯ Sigmoid å‡½æ•°çš„ä¸€ç§æ¨å¹¿ã€‚å®ƒèƒ½å°†ä¸€ä¸ªå«ä»»æ„å®æ•°çš„ K ç»´å‘é‡ $\boldsymbol{z}$Â â€œå‹ç¼©â€åˆ°å¦ä¸€ä¸ª K ç»´å®å‘é‡ $\sigma(\boldsymbol{z})$ ä¸­ï¼Œä½¿å¾—æ¯ä¸€ä¸ªå…ƒç´ çš„èŒƒå›´éƒ½åœ¨ (0, 1) ä¹‹é—´ï¼Œå¹¶ä¸”æ‰€æœ‰å…ƒç´ çš„å’Œä¸º 1ã€‚è¯¥å‡½æ•°çš„å½¢å¼é€šå¸¸æŒ‰ä¸‹é¢çš„å¼å­ç»™å‡ºï¼š
$$
\sigma(\boldsymbol{z})=\frac{1}{\sum_{i=1}^ke^{z_i}}\begin{bmatrix}
e^{z_1} \\\ 
e^{z_2} \\\ 
... \\\ 
e^{z_k}
\end{bmatrix}\quad
$$
Softmax å›å½’æ¨¡å‹å¯¹äºè¯¸å¦‚ MNIST æ‰‹å†™æ•°å­—åˆ†ç±»ç­‰é—®é¢˜å¾ˆæœ‰ç”¨ã€‚å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œç±»æ ‡è®° $y\in\lbrace0, 1\rbrace$ï¼›è€Œåœ¨ K(K > 2) åˆ†ç±»é—®é¢˜ä¸­åˆ™æ˜¯ $y\in\lbrace1, 2, â€¦, k\rbrace$ã€‚ä¾‹å¦‚ï¼Œåœ¨ MNIST æ•°å­—è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œ10 ä¸ªæ•°å­—å¯¹åº” K=10 ä¸ªä¸åŒçš„ç±»åˆ«ã€‚

å¯¹ç‡å›å½’çš„å‡è®¾å‡½æ•° $h_\boldsymbol{\theta}(\boldsymbol{x})=g(\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x})=\frac{1}{1+e^{-\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x}}}$ è®¡ç®—çš„æ˜¯æ ·æœ¬å±äºæ­£ä¾‹çš„æ¦‚ç‡ï¼Œç”±äºåªæœ‰ä¸¤ç±»ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥æ ¹æ®é˜ˆå€¼è¿›è¡Œåˆ¤æ–­ã€‚è€Œ Softmax å›å½’åˆ™éœ€è¦è¾“å‡ºä¸€ä¸ª K ç»´å‘é‡(å…ƒç´ å’Œä¸º 1ï¼Œ**å½’ä¸€åŒ–**)æ¥è¡¨ç¤ºæ ·æœ¬å±äºæ¯ä¸ªç±»çš„æ¦‚ç‡ï¼Œå› æ­¤éœ€è¦çš„æ¨¡å‹å‚æ•°ä¹Ÿå°±æ›´å¤šã€‚æœ€ååˆ¤æ–­å±äºå“ªä¸€ç±»æ—¶åˆ™å¯ä»¥å–æœ€å¤§çš„æ¦‚ç‡å€¼å¯¹åº”çš„ç±»åˆ«ã€‚
$$
h_\boldsymbol{\Theta}(\boldsymbol{x})=\begin{bmatrix}
p(y=1|\boldsymbol{x};\boldsymbol{\Theta}) \\\ 
p(y=2|\boldsymbol{x};\boldsymbol{\Theta}) \\\ 
... \\\ 
p(y=k|\boldsymbol{x};\boldsymbol{\Theta})
\end{bmatrix}=\frac{1}{\sum_{i=1}^ke^{\boldsymbol{\theta}^\mathrm{T}_i\boldsymbol{x}}}\begin{bmatrix}
e^{\boldsymbol{\theta}^\mathrm{T}_1\boldsymbol{x}} \\\ 
e^{\boldsymbol{\theta}^\mathrm{T}_2\boldsymbol{x}} \\\ 
... \\\ 
e^{\boldsymbol{\theta}^\mathrm{T}_k\boldsymbol{x}}
\end{bmatrix}\quad
å…¶ä¸­ \boldsymbol{\Theta}=\begin{bmatrix}
-\boldsymbol{\theta}^\mathrm{T}_1- \\\ 
-\boldsymbol{\theta}^\mathrm{T}_2- \\\ 
... \\\ 
-\boldsymbol{\theta}^\mathrm{T}_k-
\end{bmatrix}
$$

### ä»£ä»·å‡½æ•°

> ç¤ºæ€§å‡½æ•°ï¼š1{å€¼ä¸ºçœŸçš„è¡¨è¾¾å¼} = 1

å¯¹æ•°ä¼¼ç„¶ä»£ä»·å‡½æ•° $J(\boldsymbol{\theta}) =-\frac{1}{m} \sum_{i=1}^m(y^{(i)}log(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)}))+(1-y^{(i)})log(1-h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})))$ å¯ä»¥æ¨å¹¿ä¸ºï¼š
$$
J(\boldsymbol{\Theta}) =-\frac{1}{m}\sum_{i=1}^m\sum_{j=0}^11\lbrace y^{(i)}=j\rbrace logp(y^{(i)}=j|\boldsymbol{x}^{(i)};\boldsymbol{\Theta})
$$
å’Œå¯¹æ•°ä¾ç„¶ä¸€æ ·å¯ä»¥ç†è§£ä¸ºå½“çœŸå®æ ‡ç­¾ä¸º j æ—¶ï¼Œè¦è®©é¢„æµ‹ä¸º j ç±»çš„æ¦‚ç‡å€¼ $p(y^{(i)}=j|\boldsymbol{x}^{(i)};\boldsymbol{\Theta})$ å°½å¯èƒ½æ¥è¿‘ $1^-$ã€‚å› æ­¤ Softmax å›å½’çš„ä»£ä»·å‡½æ•°ä¸ºï¼š
$$
J(\boldsymbol{\Theta}) =-\frac{1}{m}\sum_{i=1}^m\sum_{j=1}^k1\lbrace y^{(i)}=j\rbrace log\frac{e^{\boldsymbol{\theta}\_j^\mathrm{T}\boldsymbol{x}^{(i)}}}{\sum_{l=1}^ke^{\boldsymbol{\theta}^\mathrm{T}_l\boldsymbol{x}^{(i)}}}
$$

è®¡ç®—æ¢¯åº¦å…¬å¼å¦‚ä¸‹ï¼š
$$
\begin{align}
\frac{\partial{J(\boldsymbol{\Theta})}}{\partial{\boldsymbol{\theta}\_j}} = -\frac{1}{m} \sum_{i=1}^m\left(\boldsymbol{x}^{(i)}(1\lbrace y^{(i)}=j\rbrace-\frac{e^{\boldsymbol{\theta}\_j^\mathrm{T}\boldsymbol{x}^{(i)}}}{\sum_{l=1}^ke^{\boldsymbol{\theta}^\mathrm{T}_l\boldsymbol{x}^{(i)}}})\right)
\end{align}
$$

åœ¨ä½¿ç”¨ä¸­ä¸€èˆ¬ä¼šæ·»åŠ æƒé‡è¡°å‡é¡¹(æ­£åˆ™é¡¹) $\frac{\lambda}{2}\sum_{i=1}^k\sum_{j=0}^n\theta^2_{ij}$ æƒ©ç½šè¿‡å¤§çš„å‚æ•°å€¼ï¼Œå…¶åœ¨ $\theta_j$ æ–¹å‘ä¸Šçš„æ¢¯åº¦ä¸º $\lambda\theta_j$ã€‚

## * ç¥ç»ç½‘ç»œ

> ç¥ç»ç½‘ç»œä¸­æœ€åŸºæœ¬çš„æˆåˆ†æ˜¯ç¥ç»å…ƒæ¨¡å‹ï¼Œåœ¨ç”Ÿç‰©ç¥ç»ç½‘ç»œæ˜¯ï¼Œæ¯ä¸ªç¥ç»å…ƒä¸å…¶ä»–ç¥ç»å…ƒç›¸è¿ï¼Œå½“å®ƒâ€œå…´å¥‹â€æ—¶ï¼Œå°±ä¼šå‘ç›¸è¿çš„ç¥ç»å…ƒå‘é€åŒ–å­¦ç‰©è´¨ï¼Œä»è€Œæ”¹å˜è¿™äº›ç¥ç»å…ƒå†…çš„ç”µä½ï¼›å¦‚æœæŸç¥ç»å…ƒçš„ç”µä½è¶…è¿‡äº†ä¸€ä¸ªâ€œé˜ˆå€¼â€ï¼Œé‚£ä¹ˆå®ƒå°±ä¼šè¢«æ¿€æ´»ï¼Œå³â€œå…´å¥‹â€èµ·æ¥ï¼Œå‘å…¶ä»–ç¥ç»å…ƒå‘é€åŒ–å­¦ç‰©è´¨ã€‚

æ ¹æ®ç¥ç»å…ƒçš„å®šä¹‰ï¼Œå¯ä»¥å°†å¯¹ç‡å›å½’çœ‹æˆæ˜¯ä¸€ä¸ªå¾ˆç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚å³åªæœ‰è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º(æ¥è‡ª[Tensorflow](http://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=gauss&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=40&networkShape=&seed=0.49707&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&stepButton_hide=false&noise_hide=false))ï¼š

![Logistic regression](https://randy-1251769892.cos.ap-beijing.myqcloud.com/logistic-regression.gif)

æœ‰éšè—å±‚çš„ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚å°±æ˜¯ä¸€ä¸ªå¯¹ç‡å›å½’ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ã€‚è¾“å…¥å±‚å’Œä¸­é—´çš„éšè—å±‚å¯ä»¥çœ‹æˆç‰¹å¾æå–çš„è¿‡ç¨‹ï¼Œå°±æ˜¯æŠŠå¯¹ç‡å›å½’çš„è¾“å‡ºå½“ä½œç‰¹å¾ï¼Œç„¶åå†å°†å®ƒé€å…¥ä¸‹ä¸€ä¸ªå¯¹ç‡å›å½’ï¼Œä¸€å±‚å±‚å˜æ¢ã€‚ç”±äºæ¿€æ´»å‡½æ•°æ˜¯éçº¿æ€§å‡½æ•°ï¼Œæ‰€ä»¥é€šè¿‡ç‰¹å¾æå–ï¼Œå°±å¯ä»¥æŠŠåŸæœ¬çº¿æ€§ä¸å¯åˆ†çš„æ•°æ®å˜å¾—çº¿æ€§å¯åˆ†ã€‚

## å‚è€ƒæ–‡çŒ®

[1] å‘¨å¿—å. æœºå™¨å­¦ä¹ . æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾.  2016.

[2] å´æ©è¾¾. DeepLearning. 

[3] Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾. 2017.

[4] Stephen Boyd, Lieven Vandenberghe. å‡¸ä¼˜åŒ–. æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾. 2017.

[5] Christopher M.Bishop. Pattern Recognition and Machine Learning. 2006.